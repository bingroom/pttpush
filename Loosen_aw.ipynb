{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Trie..., from /home/joekaojoekao/github/dict.txt.big\n",
      "DEBUG:jieba:Building Trie..., from /home/joekaojoekao/github/dict.txt.big\n",
      "loading model from cache /tmp/jieba.user.8815186977980235228.cache\n",
      "DEBUG:jieba:loading model from cache /tmp/jieba.user.8815186977980235228.cache\n",
      "loading model cost 3.02387714386 seconds.\n",
      "DEBUG:jieba:loading model cost 3.02387714386 seconds.\n",
      "Trie has been built succesfully.\n",
      "DEBUG:jieba:Trie has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read source: ./visualized/select_pushes1000_1.txt\n",
      "Pushes filtering (keep only pushes larger than 4 words)\n",
      "After filtering / Origin push num:  999 / 999\n",
      "Users filtering (remove users who post less than 3 pushes)\n",
      "After filtering / Previous user num:  718 / 999\n",
      "write user list: ./temp/select_pushes1000_1_userlist.txt\n",
      "write push data ready to do cross_validation: ./temp/select_pushes1000_1_cvlist.txt\n"
     ]
    }
   ],
   "source": [
    "%run gen_cv.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read user from  ./temp/select_pushes1000_1_userlist.txt\n",
      "read push from ./temp/select_pushes1000_1_cvlist.txt\n",
      "0\n",
      "-------\n",
      "Num of aw words: 5964\n",
      "lt:0.00001 178 2416\n",
      "lt:0.00010 178 2416\n",
      "lt:0.00100 178 2159\n",
      "lt:0.01000 114 279\n",
      "write to  ./cv_result/simlist_2015-07-16_0144/select_pushes1000_1_cv0.6.txt\n",
      "-------\n",
      "Num of aw words: 7241\n",
      "lt:0.00001 246 5083\n",
      "lt:0.00010 246 5083\n",
      "lt:0.00100 246 4668\n",
      "lt:0.01000 180 595\n",
      "write to  ./cv_result/simlist_2015-07-16_0144/select_pushes1000_1_cv0.75.txt\n",
      "-------\n",
      "Num of aw words: 7851\n",
      "lt:0.00001 281 7975\n",
      "lt:0.00010 281 7975\n",
      "lt:0.00100 281 7408\n",
      "lt:0.01000 216 850\n",
      "write to  ./cv_result/simlist_2015-07-16_0144/select_pushes1000_1_cv0.8.txt\n",
      "-------\n",
      "Num of aw words: 8201\n",
      "lt:0.00001 295 11048\n",
      "lt:0.00010 295 11048\n",
      "lt:0.00100 295 10351\n",
      "lt:0.01000 237 1125\n",
      "write to  ./cv_result/simlist_2015-07-16_0144/select_pushes1000_1_cv0.85.txt\n",
      "-------\n",
      "Num of aw words: 8419\n",
      "lt:0.00001 306 14016\n",
      "lt:0.00010 306 14016\n",
      "lt:0.00100 306 13178\n",
      "lt:0.01000 255 1393\n",
      "write to  ./cv_result/simlist_2015-07-16_0144/select_pushes1000_1_cv0.9.txt\n",
      "-------\n",
      "Num of aw words: 8923\n",
      "lt:0.00001 352 30614\n",
      "lt:0.00010 352 30614\n",
      "lt:0.00100 352 28806\n",
      "lt:0.01000 304 3254\n",
      "write to  ./cv_result/simlist_2015-07-16_0144/select_pushes1000_1_cv0.95.txt\n"
     ]
    }
   ],
   "source": [
    "%%pypy\n",
    "import numpypy as np # while at sweslos' centos6.x\n",
    "\n",
    "## using pypy magic needs a temp file for output \n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def count_dict(sample_dict_jieba):\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "    import itertools\n",
    "    ptt_pushes_freq_bypush = {}\n",
    "    for uid, push_list in sample_dict_jieba.iteritems():\n",
    "        push = push_list[0] #raw push\n",
    "        a = zip(*push_list)[1] #jieba ones\n",
    "        push_gram = list(itertools.chain(*a))\n",
    "        count = Counter(push_gram)\n",
    "        temp = []\n",
    "        for w, c in count.most_common():\n",
    "            temp.append((w, c))\n",
    "        id_count = Counter(dict(temp))\n",
    "        ptt_pushes_freq_bypush[uid] = dict(id_count)\n",
    "\n",
    "    return ptt_pushes_freq_bypush\n",
    "\n",
    "def weighted_jaccard(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        return -1\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in xrange(len(l1)):\n",
    "        num += np.minimum(l1[i], l2[i])\n",
    "        den += np.maximum(l1[i], l2[i])\n",
    "    wj = np.divide(np.float64(num), den+1)\n",
    "\n",
    "    return wj\n",
    "\n",
    "import os\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_file = config[\"sample_file\"] #first input\n",
    "temp_path = config[\"temp_path\"] \n",
    "result_dir = config[\"result_dir\"]\n",
    "\n",
    "## just pick some user here from temp folder\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt', 'rb') as f_temp:\n",
    "    print 'read user from ', temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt'\n",
    "    user_list = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt', 'rb') as f_temp:\n",
    "    print 'read push from', temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt'\n",
    "    temp_json = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "    \n",
    "\n",
    "dict_train, dict_test = temp_json # train/test push list for each user\n",
    "\n",
    "dict_train_count = count_dict(dict_train) #count the freq\n",
    "dict_test_count = count_dict(dict_test) #count the freq for ans(test)\n",
    "\n",
    "from collections import Counter\n",
    "count_all = Counter()\n",
    "for v in dict_train_count.values():\n",
    "    count_all += Counter(v)\n",
    "\n",
    "from collections import OrderedDict\n",
    "sorted_tuple_list_train_count = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "## global term count using train data\n",
    "\n",
    "# with open(temp_path + os.path.splitext(sample_file)[0] + '_train_count.txt', 'wb') as f_temp:\n",
    "#     f_temp.write(json.dumps(dict_train_count, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "#     f_temp.close()\n",
    "\n",
    "################\n",
    "\n",
    "##test function\n",
    "\n",
    "#################Feature Extraction##########################\n",
    "\n",
    "def loosen_AW(dict_train_count, begin, itvl, K_NUMWORD=6, show=False):\n",
    "    ###if need to show, don't use pypy (pypy can't plot)\n",
    "    print '0'\n",
    "    def _get_aw(tlist, word, wp, K_NUMWORD):\n",
    "        W_PERCENT = wp\n",
    "        author_words = [x for x in word if tlist[x] <= tlist[word[int(len(word) * (1-W_PERCENT))]]] #0.975\n",
    "        ## stop word list\n",
    "        aw_list = [x for x in author_words] \n",
    "        aw_count_dict = {}\n",
    "        for w in aw_list:\n",
    "            aw_count_dict.setdefault(len(w),[]).append(w)\n",
    "        j = {}\n",
    "        K_NUMWORD = 6 #6\n",
    "        selected_aw = []\n",
    "        for k in aw_count_dict.keys()[0:K_NUMWORD]:\n",
    "            #print 'num_word:',k,'\\t',len(aw_count_dict[k])\n",
    "            selected_aw += aw_count_dict[k]\n",
    "        #print 'words (selected / rare / total)', len(selected_aw), '/', len(aw_list), '/', len(word) \n",
    "        return selected_aw\n",
    "    \n",
    "    ##get all word count\n",
    "    from collections import Counter\n",
    "    count_all = Counter()\n",
    "    for v in dict_train_count.values():\n",
    "        count_all += Counter(v)\n",
    "    from collections import OrderedDict\n",
    "    tlist = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "    \n",
    "    word = tlist.keys()\n",
    "    \n",
    "    \n",
    "    #touch W_PERCENT\n",
    "    aw_len_list = []\n",
    "    for wp in np.arange(begin, 1.0 + itvl, itvl):\n",
    "        selected_aw = _get_aw(tlist, word, wp, K_NUMWORD)\n",
    "        #selected_aw = selected_aw[0: len(selected_aw)/2]\n",
    "        aw_len_list.append((wp,len(selected_aw), selected_aw))\n",
    "        \"get slope change point from high to low\"\n",
    "    wp = zip(*aw_len_list)[0]\n",
    "    aw_len = zip(*aw_len_list)[1]\n",
    "    aw_list = zip(*aw_len_list)[2]\n",
    "    \n",
    "    #get gradient slope\n",
    "    slope = []\n",
    "    for i in xrange(1, len(aw_len)):\n",
    "        slope.append(aw_len[i] - aw_len[i-1])\n",
    "    sc = [(wp[i], aw_len[i], aw_list[i]) for i, x in enumerate(slope) if x != 0]\n",
    "    \n",
    "    #check the graph\n",
    "    if show:\n",
    "        plt.plot(*zip(*aw_len_list)) \n",
    "        plt.show()\n",
    "        print slope\n",
    "        print sc\n",
    "    return sc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ls_AW = loosen_AW(dict_train_count, 0.1, 0.05)\n",
    "\n",
    "dirfmt = \"simlist_%4d-%02d-%02d_%02d%02d\"\n",
    "now = time.localtime()[0:5]\n",
    "dirname = dirfmt % now\n",
    "loosen_dir = result_dir + dirname\n",
    "os.mkdir(loosen_dir)\n",
    "\n",
    "\n",
    "\n",
    "def _get_ans_by_sim(sorted_usl, lt_value):\n",
    "    LT_VALUE = lt_value\n",
    "    ans_num = sum(sim > LT_VALUE and user_i+'ANS' == user_j for user_i, user_j, sim in sorted_usl)\n",
    "    return ans_num\n",
    "def _get_user_by_sim(sorted_usl, lt_value):\n",
    "    LT_VALUE = lt_value\n",
    "    user_num = sum(sim > LT_VALUE for sim in zip(*sorted_usl)[2])\n",
    "    return user_num\n",
    "    \n",
    "\n",
    "for wp, aw, aw_list in ls_AW:\n",
    "    general_vec = {}\n",
    "    for uid in user_list: # for each user id\n",
    "        user_len = sum([len(x) for x in dict_train_count[uid]]) #total word freq\n",
    "        if user_len > 0:\n",
    "            vec = [dict_train_count[uid].get(w, 0) for w in aw_list] #aw_list[0] is bug\n",
    "            g_vec = [float(x) / user_len for x in vec]\n",
    "            general_vec[uid] = g_vec\n",
    "\n",
    "\n",
    "    general_vec_ans = {}\n",
    "    for uid in user_list: # for each user id\n",
    "        user_len = sum([len(x) for x in dict_test_count[uid]]) #total word freq\n",
    "        #print sum(v.values())\n",
    "        if user_len > 0:\n",
    "            vec = [dict_test_count[uid].get(w, 0) for w in aw_list]\n",
    "            g_vec = [float(x) / user_len for x in vec]\n",
    "            general_vec_ans[uid] = g_vec\n",
    "\n",
    "    # print user_list[0]\n",
    "    # print general_vec[user_list[0]]\n",
    "    # print general_vec_ans[user_list[0]]\n",
    "\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    user_size = len(user_list)\n",
    "    #print user_size\n",
    "\n",
    "    sim_list = np.array(np.arange(user_size*user_size))\n",
    "    sim_list = sim_list.astype(float)\n",
    "    print '-------'\n",
    "    \n",
    "    idx = 0\n",
    "    for i in xrange(user_size):\n",
    "        for j in xrange(user_size):\n",
    "            #wj_sw = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "            #sim_list = np.vstack((sim_list, np.array((round(float(i),1), round(float(j),1), wj_sw))))\n",
    "\n",
    "            sim_list[idx] = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "            idx += 1\n",
    "    sim_list = sim_list[:idx]\n",
    "    t_stop = time.time()\n",
    "    #print sim_list\n",
    "    #print 'sim compare time = ', t_stop - t_start, 's'\n",
    "\n",
    "    user_sim_list = []\n",
    "    idx = 0\n",
    "    for i in xrange(user_size):\n",
    "        for j in xrange(user_size):\n",
    "            sim = sim_list[idx]\n",
    "            idx += 1\n",
    "            if str(sim) == '0.0':\n",
    "                continue\n",
    "            user_sim_list.append((user_list[int(i)], user_list[int(j)] + 'ANS', sim))\n",
    "    \n",
    "    from operator import itemgetter \n",
    "    sorted_usl = sorted(user_sim_list, key=itemgetter(2), reverse=True)\n",
    "    print 'Num of aw words:', aw\n",
    "    print 'lt:0.00001', _get_ans_by_sim(sorted_usl, 0.00001) , _get_user_by_sim(sorted_usl, 0.00001)\n",
    "    print 'lt:0.00010', _get_ans_by_sim(sorted_usl, 0.0001) , _get_user_by_sim(sorted_usl, 0.0001)\n",
    "    print 'lt:0.00100', _get_ans_by_sim(sorted_usl, 0.001) , _get_user_by_sim(sorted_usl, 0.001)\n",
    "    print 'lt:0.01000', _get_ans_by_sim(sorted_usl, 0.01) , _get_user_by_sim(sorted_usl, 0.01)\n",
    "    \n",
    "    #name = os.path.splitext(sample_file)[0]\n",
    "    resultpath = loosen_dir + '/' + os.path.splitext(sample_file)[0] + '_cv' + str(wp) + '.txt'\n",
    "    with open(resultpath, 'wb') as fout:\n",
    "        \n",
    "        print 'write to ', resultpath \n",
    "        for user, ans, sim in sorted_usl:\n",
    "            line = user + ',' + ans + ',' + str(sim) +'\\n'\n",
    "            fout.write(line.encode('utf-8'))\n",
    "        fout.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lt:0.0000 2\n"
     ]
    }
   ],
   "source": [
    "def _get_ans_by_sim(sorted_usl, lt_value):\n",
    "    LT_VALUE = lt_value\n",
    "    \n",
    "    return sum(sim > LT_VALUE and user_i+'ANS' == user_j for user_i, user_j, sim in sorted_usl)\n",
    "sorted_usl = [['i','iANS', 0.9],['j','jANS', 0.8]]\n",
    "print 'lt:0.0000', _get_ans_by_sim(sorted_usl, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1, 5964], [0.11, 5964], [0.12, 5964], [0.13, 5964], [0.14, 5964], [0.15, 5964], [0.16, 5964], [0.17, 5964], [0.18, 5964], [0.19, 5964], [0.2, 5964], [0.21, 5964], [0.22, 5964], [0.23, 5964], [0.24, 5964], [0.25, 5964], [0.26, 5964], [0.27, 5964], [0.28, 5964], [0.29, 5964], [0.3, 5964], [0.31, 5964], [0.32, 5964], [0.33, 5964], [0.34, 5964], [0.35, 5964], [0.36, 5964], [0.37, 5964], [0.38, 5964], [0.39, 5964], [0.4, 5964], [0.41, 5964], [0.42, 5964], [0.43, 5964], [0.44, 5964], [0.45, 5964], [0.46, 5964], [0.47, 5964], [0.48, 5964], [0.49, 5964], [0.5, 5964], [0.51, 5964], [0.52, 5964], [0.53, 5964], [0.54, 5964], [0.55, 5964], [0.56, 5964], [0.57, 5964], [0.58, 5964], [0.59, 5964], [0.6, 5964], [0.61, 5964], [0.62, 5964], [0.63, 5964], [0.64, 7241], [0.65, 7241], [0.66, 7241], [0.67, 7241], [0.68, 7241], [0.69, 7241], [0.7, 7241], [0.71, 7241], [0.72, 7241], [0.73, 7241], [0.74, 7241], [0.75, 7241], [0.76, 7241], [0.77, 7241], [0.78, 7851], [0.79, 7851], [0.8, 7851], [0.81, 7851], [0.82, 7851], [0.83, 7851], [0.84, 7851], [0.85, 8201], [0.86, 8201], [0.87, 8201], [0.88, 8419], [0.89, 8419], [0.9, 8419], [0.91, 8563], [0.92, 8668], [0.93, 8738], [0.94, 8802], [0.95, 8923], [0.96, 8986], [0.97, 9073], [0.98, 9174], [0.99, 9263], [1.0, 9358]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21417e21d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 1277    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  610    0    0    0    0    0    0  350\n",
      "    0    0  218    0    0  144  105   70   64  121   63   87  101   89   95]\n",
      "[(0.64, 7241), (0.78, 7851), (0.85, 8201), (0.88, 8419), (0.91, 8563), (0.92, 8668), (0.93, 8738), (0.94, 8802), (0.95, 8923), (0.96, 8986), (0.97, 9073), (0.98, 9174), (0.99, 9263), (1.0, 9358)]\n",
      "[(1.0, 9358), (0.99, 9263), (0.98, 9174), (0.97, 9073), (0.96, 8986), (0.95, 8923), (0.94, 8802), (0.93, 8738), (0.92, 8668), (0.91, 8563), (0.88, 8419), (0.85, 8201), (0.78, 7851), (0.64, 7241)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_file = config[\"sample_file\"] #first input\n",
    "temp_path = config[\"temp_path\"]\n",
    "\n",
    "import json\n",
    "with open(temp_path + 'AW_test.txt', \"rb\") as f_temp:\n",
    "    aw_len_list = json.load(f_temp)\n",
    "print aw_len_list\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(aw_len_list)\n",
    "plt.plot(*zip(*aw_len_list))\n",
    "plt.show()\n",
    "idx = zip(*aw_len_list)[0]\n",
    "aw_len = zip(*aw_len_list)[1]\n",
    "slope = np.diff(aw_len[::1])\n",
    "print slope\n",
    "sc = [(idx[i+1], aw_len[i+1]) for i, x in enumerate(slope) if x != 0]\n",
    "print sc\n",
    "print sc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loosen_by_sc(aw_len_list, show=False):\n",
    "    \"get slope change point from high to low\"\n",
    "    \n",
    "    idx = zip(*aw_len_list)[0]\n",
    "    aw_len = zip(*aw_len_list)[1]\n",
    "    slope = np.diff(aw_len[::1])\n",
    "    sc = [(idx[i+1], aw_len[i+1]) for i, x in enumerate(slope) if x != 0]\n",
    "    if show:\n",
    "        plt.plot(*zip(*aw_len_list)) \n",
    "        plt.show()\n",
    "        print slope\n",
    "        print sc\n",
    "    return sc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read user from  ./temp/select_pushes1000_1_userlist.txt\n",
      "read push from ./temp/select_pushes1000_1_cvlist.txt\n",
      "total word 9507\n",
      "rare word 6073\n",
      "num_word: 1 \t447\n",
      "num_word: 2 \t4048\n",
      "num_word: 3 \t991\n",
      "num_word: 4 \t394\n",
      "num_word: 5 \t50\n",
      "num_word: 6 \t34\n",
      "5964\n",
      "5964\n",
      "718\n",
      "...\n",
      "sim compare time =  8.23145008087 s\n",
      "write to  ./cv_result/select_pushes1000_1_cv.txt\n"
     ]
    }
   ],
   "source": [
    "%%pypy \n",
    "## using pypy magic needs a temp file for output \n",
    "import json\n",
    "import numpypy as np # while at sweslos' centos6.x\n",
    "import time\n",
    "\n",
    "def count_dict(sample_dict_jieba):\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "    import itertools\n",
    "    ptt_pushes_freq_bypush = {}\n",
    "    for uid, push_list in sample_dict_jieba.iteritems():\n",
    "        push = push_list[0] #raw push\n",
    "        a = zip(*push_list)[1] #jieba ones\n",
    "        push_gram = list(itertools.chain(*a))\n",
    "        count = Counter(push_gram)\n",
    "        temp = []\n",
    "        for w, c in count.most_common():\n",
    "            temp.append((w, c))\n",
    "        id_count = Counter(dict(temp))\n",
    "        ptt_pushes_freq_bypush[uid] = dict(id_count)\n",
    "\n",
    "    return ptt_pushes_freq_bypush\n",
    "\n",
    "def weighted_jaccard(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        return -1\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in xrange(len(l1)):\n",
    "        num += np.minimum(l1[i], l2[i])\n",
    "        den += np.maximum(l1[i], l2[i])\n",
    "    wj = np.divide(np.float64(num), den+1)\n",
    "\n",
    "    return wj\n",
    "\n",
    "import os\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_file = config[\"sample_file\"] #first input\n",
    "temp_path = config[\"temp_path\"] \n",
    "result_dir = config[\"result_dir\"]\n",
    "\n",
    "## just pick some user here from temp folder\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt', 'rb') as f_temp:\n",
    "    print 'read user from ', temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt'\n",
    "    user_list = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt', 'rb') as f_temp:\n",
    "    print 'read push from', temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt'\n",
    "    temp_json = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "    \n",
    "\n",
    "dict_train, dict_test = temp_json # train/test push list for each user\n",
    "\n",
    "dict_train_count = count_dict(dict_train) #count the freq\n",
    "dict_test_count = count_dict(dict_test) #count the freq for ans(test)\n",
    "\n",
    "from collections import Counter\n",
    "count_all = Counter()\n",
    "for v in dict_train_count.values():\n",
    "    count_all += Counter(v)\n",
    "\n",
    "from collections import OrderedDict\n",
    "sorted_tuple_list_train_count = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "## global term count using train data\n",
    "\n",
    "# with open(temp_path + os.path.splitext(sample_file)[0] + '_train_count.txt', 'wb') as f_temp:\n",
    "#     f_temp.write(json.dumps(dict_train_count, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "#     f_temp.close()\n",
    "\n",
    "################\n",
    "\n",
    "##test function\n",
    "\n",
    "#################Feature Extraction##########################\n",
    "\n",
    "def AW(dict_train_count, W_PERCENT): #content word\n",
    "\n",
    "    from collections import Counter\n",
    "    count_all = Counter()\n",
    "    for v in dict_train_count.values():\n",
    "        count_all += Counter(v)\n",
    "\n",
    "    from collections import OrderedDict\n",
    "    tlist = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "    #sorted_tuple_list_train_count\n",
    "    ##gen sw (but only from training data)\n",
    "    word = tlist.keys()\n",
    "    #print dict(sorted_tuple_list_train_count)\n",
    "    #W_PERCENT = 0.025 #0.025\n",
    "    ## setting for stopword & rareword percentage\n",
    "    author_words = [x for x in word if tlist[x] <= tlist[word[int(len(word) * (1-W_PERCENT))]]] #0.975\n",
    "\n",
    "    ## stop word list\n",
    "    aw_list = [x for x in author_words] \n",
    "    ## rare word list\n",
    "    #rw_list = [x for x in rarewords] \n",
    "    print 'total word', len(word)\n",
    "    print 'rare word', len(aw_list)\n",
    "    #print 'rare word', len(rw_list)\n",
    "\n",
    "\n",
    "    aw_count_dict = {}\n",
    "    for w in aw_list:\n",
    "        aw_count_dict.setdefault(len(w),[]).append(w)\n",
    "\n",
    "\n",
    "    j = {}\n",
    "    K_NUMWORD = 6 #6\n",
    "    selected_aw = []\n",
    "    for k in aw_count_dict.keys()[0:K_NUMWORD]:\n",
    "        print 'num_word:',k,'\\t',len(aw_count_dict[k])\n",
    "        #j[k] = aw_count_dict[k]\n",
    "        #print json.dumps(j[1])\n",
    "        selected_aw += aw_count_dict[k]\n",
    "    print len(selected_aw)\n",
    "    \n",
    "    return selected_aw\n",
    "\n",
    "selected_aw = AW(dict_train_count, 0.025)\n",
    "print len(selected_aw)\n",
    "    \n",
    "\n",
    "# with open('./feature_pool/' + 'CW.txt', 'wb') as f_feature:\n",
    "#     f_feature.write(json.dumps(selected_cw, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "#     f_feature.close()\n",
    "\n",
    "###############VEC TIME##########################################\n",
    "\n",
    "## 2. gen aw vec\n",
    "import itertools\n",
    "general_vec = {}\n",
    "for uid in user_list: # for each user id\n",
    "    user_len = sum([len(x) for x in dict_train_count[uid]]) #total word freq\n",
    "    if user_len > 0:\n",
    "        vec = [dict_train_count[uid].get(w, 0) for w in selected_aw]\n",
    "        g_vec = [float(x) / user_len for x in vec]\n",
    "        general_vec[uid] = g_vec\n",
    "\n",
    "\n",
    "general_vec_ans = {}\n",
    "for uid in user_list: # for each user id\n",
    "    user_len = sum([len(x) for x in dict_test_count[uid]]) #total word freq\n",
    "    #print sum(v.values())\n",
    "    if user_len > 0:\n",
    "        vec = [dict_test_count[uid].get(w, 0) for w in selected_aw]\n",
    "        g_vec = [float(x) / user_len for x in vec]\n",
    "        general_vec_ans[uid] = g_vec\n",
    "\n",
    "# print user_list[0]\n",
    "# print general_vec[user_list[0]]\n",
    "# print general_vec_ans[user_list[0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "user_size = len(user_list)\n",
    "print user_size\n",
    "\n",
    "sim_list = np.array(np.arange(user_size*user_size))\n",
    "sim_list = sim_list.astype(float)\n",
    "print '...'\n",
    "idx = 0\n",
    "for i in xrange(user_size):\n",
    "    for j in xrange(user_size):\n",
    "        #wj_sw = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "        #sim_list = np.vstack((sim_list, np.array((round(float(i),1), round(float(j),1), wj_sw))))\n",
    "        sim_list[idx] = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "        idx += 1\n",
    "sim_list = sim_list[:idx]\n",
    "t_stop = time.time()\n",
    "print 'sim compare time = ', t_stop - t_start, 's'\n",
    "\n",
    "user_sim_list = []\n",
    "idx = 0\n",
    "for i in xrange(user_size):\n",
    "    for j in xrange(user_size):\n",
    "        sim = sim_list[idx]\n",
    "        idx += 1\n",
    "        user_sim_list.append((user_list[int(i)], user_list[int(j)] + 'ANS', sim))\n",
    "with open(result_dir + os.path.splitext(sample_file)[0] + '_cv.txt', 'wb') as fout:\n",
    "    resultpath = result_dir + os.path.splitext(sample_file)[0] + '_cv.txt'\n",
    "    print 'write to ', resultpath \n",
    "    for user, ans, sim in user_sim_list:\n",
    "        line = user + ',' + ans + ',' + str(sim) +'\\n'\n",
    "        fout.write(line.encode('utf-8'))\n",
    "    fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read user from  ./temp/select_pushes1000_1_userlist.txt\n",
      "read push from ./temp/select_pushes1000_1_cvlist.txt\n",
      "-------\n",
      "Num of aw words: 8792\n",
      "lt: 0.1 215 3309\n",
      "lt: 0.2 62 226\n",
      "lt: 0.3 30 50\n",
      "lt: 0.4 20 21\n",
      "lt: 0.5 2 2\n",
      "write to  ./cv_result/simlist_2015-07-15_1204/select_pushes1000_1_cv0.6.txt\n",
      "-------\n",
      "Num of aw words: 3306\n",
      "lt: 0.1 215 3309\n",
      "lt: 0.2 62 226\n",
      "lt: 0.3 30 50\n",
      "lt: 0.4 20 21\n",
      "lt: 0.5 2 2\n",
      "write to  ./cv_result/simlist_2015-07-15_1204/select_pushes1000_1_cv0.75.txt\n",
      "-------\n",
      "Num of aw words: 2082\n",
      "lt: 0.1 215 3309\n",
      "lt: 0.2 62 226\n",
      "lt: 0.3 30 50\n",
      "lt: 0.4 20 21\n",
      "lt: 0.5 2 2\n",
      "write to  ./cv_result/simlist_2015-07-15_1204/select_pushes1000_1_cv0.8.txt\n",
      "-------\n",
      "Num of aw words: 1488\n",
      "lt: 0.1 215 3309\n",
      "lt: 0.2 62 226\n",
      "lt: 0.3 30 50\n",
      "lt: 0.4 20 21\n",
      "lt: 0.5 2 2\n",
      "write to  ./cv_result/simlist_2015-07-15_1204/select_pushes1000_1_cv0.85.txt\n",
      "-------\n",
      "Num of aw words: 1142\n",
      "lt: 0.1 215 3309\n",
      "lt: 0.2 62 226\n",
      "lt: 0.3 30 50\n",
      "lt: 0.4 20 21\n",
      "lt: 0.5 2 2\n",
      "write to  ./cv_result/simlist_2015-07-15_1204/select_pushes1000_1_cv0.9.txt\n",
      "-------\n",
      "Num of aw words: 471\n",
      "lt: 0.1 215 3309\n",
      "lt: 0.2 62 226\n",
      "lt: 0.3 30 50\n",
      "lt: 0.4 20 21\n",
      "lt: 0.5 2 2\n",
      "write to  ./cv_result/simlist_2015-07-15_1204/select_pushes1000_1_cv0.95.txt\n"
     ]
    }
   ],
   "source": [
    "%%pypy\n",
    "import numpypy as np # while at sweslos' centos6.x\n",
    "\n",
    "## using pypy magic needs a temp file for output \n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def count_dict(sample_dict_jieba):\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "    import itertools\n",
    "    ptt_pushes_freq_bypush = {}\n",
    "    for uid, push_list in sample_dict_jieba.iteritems():\n",
    "        push = push_list[0] #raw push\n",
    "        a = zip(*push_list)[1] #jieba ones\n",
    "        push_gram = list(itertools.chain(*a))\n",
    "        count = Counter(push_gram)\n",
    "        temp = []\n",
    "        for w, c in count.most_common():\n",
    "            temp.append((w, c))\n",
    "        id_count = Counter(dict(temp))\n",
    "        ptt_pushes_freq_bypush[uid] = dict(id_count)\n",
    "\n",
    "    return ptt_pushes_freq_bypush\n",
    "\n",
    "def weighted_jaccard(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        return -1\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in xrange(len(l1)):\n",
    "        num += np.minimum(l1[i], l2[i])\n",
    "        den += np.maximum(l1[i], l2[i])\n",
    "    wj = np.divide(np.float64(num), den+1)\n",
    "\n",
    "    return wj\n",
    "\n",
    "import os\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_file = config[\"sample_file\"] #first input\n",
    "temp_path = config[\"temp_path\"] \n",
    "result_dir = config[\"result_dir\"]\n",
    "\n",
    "## just pick some user here from temp folder\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt', 'rb') as f_temp:\n",
    "    print 'read user from ', temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt'\n",
    "    user_list = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt', 'rb') as f_temp:\n",
    "    print 'read push from', temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt'\n",
    "    temp_json = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "    \n",
    "\n",
    "dict_train, dict_test = temp_json # train/test push list for each user\n",
    "\n",
    "dict_train_count = count_dict(dict_train) #count the freq\n",
    "dict_test_count = count_dict(dict_test) #count the freq for ans(test)\n",
    "\n",
    "from collections import Counter\n",
    "count_all = Counter()\n",
    "for v in dict_train_count.values():\n",
    "    count_all += Counter(v)\n",
    "\n",
    "from collections import OrderedDict\n",
    "sorted_tuple_list_train_count = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "## global term count using train data\n",
    "\n",
    "# with open(temp_path + os.path.splitext(sample_file)[0] + '_train_count.txt', 'wb') as f_temp:\n",
    "#     f_temp.write(json.dumps(dict_train_count, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "#     f_temp.close()\n",
    "\n",
    "################\n",
    "\n",
    "##test function\n",
    "\n",
    "#################Feature Extraction##########################\n",
    "\n",
    "def loosen_AW(dict_train_count, begin, itvl, K_NUMWORD=6, show=False):\n",
    "    ###if need to show, don't use pypy (pypy can't plot)\n",
    "    \n",
    "    def _get_aw(tlist, word, wp, K_NUMWORD):\n",
    "        W_PERCENT = wp\n",
    "        author_words = [x for x in word if tlist[x] >= tlist[word[int(len(word) * (1-W_PERCENT))]]] #0.975\n",
    "        ## stop word list\n",
    "        aw_list = [x for x in author_words] \n",
    "        aw_count_dict = {}\n",
    "        for w in aw_list:\n",
    "            aw_count_dict.setdefault(len(w),[]).append(w)\n",
    "        j = {}\n",
    "        K_NUMWORD = 3 #6\n",
    "        selected_aw = []\n",
    "        for k in aw_count_dict.keys()[0:K_NUMWORD]:\n",
    "            #print 'num_word:',k,'\\t',len(aw_count_dict[k])\n",
    "            selected_aw += aw_count_dict[k]\n",
    "        #print 'words (selected / rare / total)', len(selected_aw), '/', len(aw_list), '/', len(word) \n",
    "        return selected_aw\n",
    "\n",
    "    def _get_ans_by_sim(sorted_usl, lt_value):\n",
    "        LT_VALUE = lt_value\n",
    "        return sum(sim > LT_VALUE and user_i+'ANS' == user_j for user_i, user_j, sim in sorted_usl)\n",
    "    \n",
    "    def _get_user_by_sim(sorted_usl, lt_value):\n",
    "        LT_VALUE = lt_value\n",
    "        return sum(sim > LT_VALUE for sim in zip(*sorted_usl)[2])\n",
    "    \n",
    "\n",
    "    ##get all word count\n",
    "    from collections import Counter\n",
    "    count_all = Counter()\n",
    "    for v in dict_train_count.values():\n",
    "        count_all += Counter(v)\n",
    "    from collections import OrderedDict\n",
    "    tlist = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "    \n",
    "    word = tlist.keys()\n",
    "    \n",
    "    \n",
    "    #touch W_PERCENT\n",
    "    aw_len_list = []\n",
    "    for wp in np.arange(begin, 1.0 + itvl, itvl):\n",
    "        selected_aw = _get_aw(tlist, word, wp, K_NUMWORD)\n",
    "        #selected_aw = selected_aw[0: len(selected_aw)/2]\n",
    "        aw_len_list.append((wp,len(selected_aw), selected_aw))\n",
    "        \"get slope change point from high to low\"\n",
    "    wp = zip(*aw_len_list)[0]\n",
    "    aw_len = zip(*aw_len_list)[1]\n",
    "    aw_list = zip(*aw_len_list)[2]\n",
    "    \n",
    "    #get gradient slope\n",
    "    slope = []\n",
    "    for i in xrange(1, len(aw_len)):\n",
    "        slope.append(aw_len[i] - aw_len[i-1])\n",
    "    sc = [(wp[i], aw_len[i], aw_list) for i, x in enumerate(slope) if x != 0]\n",
    "    \n",
    "    #check the graph\n",
    "    if show:\n",
    "        plt.plot(*zip(*aw_len_list)) \n",
    "        plt.show()\n",
    "        print slope\n",
    "        print sc\n",
    "    return sc\n",
    "\n",
    "ls_AW = loosen_AW(dict_train_count, 0.6, 0.05)\n",
    "\n",
    "\n",
    "dirfmt = \"simlist_%4d-%02d-%02d_%02d%02d\"\n",
    "now = time.localtime()[0:5]\n",
    "dirname = dirfmt % now\n",
    "loosen_dir = result_dir + dirname\n",
    "os.mkdir(loosen_dir)\n",
    "\n",
    "for wp, aw, aw_list in ls_AW:\n",
    "    general_vec = {}\n",
    "    for uid in user_list: # for each user id\n",
    "        user_len = sum([len(x) for x in dict_train_count[uid]]) #total word freq\n",
    "        if user_len > 0:\n",
    "            vec = [dict_train_count[uid].get(w, 0) for w in aw_list[0]] #aw_list[0] is bug\n",
    "            g_vec = [float(x) / user_len for x in vec]\n",
    "            general_vec[uid] = g_vec\n",
    "\n",
    "\n",
    "    general_vec_ans = {}\n",
    "    for uid in user_list: # for each user id\n",
    "        user_len = sum([len(x) for x in dict_test_count[uid]]) #total word freq\n",
    "        #print sum(v.values())\n",
    "        if user_len > 0:\n",
    "            vec = [dict_test_count[uid].get(w, 0) for w in aw_list[0]]\n",
    "            g_vec = [float(x) / user_len for x in vec]\n",
    "            general_vec_ans[uid] = g_vec\n",
    "\n",
    "    # print user_list[0]\n",
    "    # print general_vec[user_list[0]]\n",
    "    # print general_vec_ans[user_list[0]]\n",
    "\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    user_size = len(user_list)\n",
    "    #print user_size\n",
    "\n",
    "    sim_list = np.array(np.arange(user_size*user_size))\n",
    "    sim_list = sim_list.astype(float)\n",
    "    print '-------'\n",
    "    \n",
    "    idx = 0\n",
    "    for i in xrange(user_size):\n",
    "        for j in xrange(user_size):\n",
    "            #wj_sw = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "            #sim_list = np.vstack((sim_list, np.array((round(float(i),1), round(float(j),1), wj_sw))))\n",
    "\n",
    "            sim_list[idx] = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "            idx += 1\n",
    "    sim_list = sim_list[:idx]\n",
    "    t_stop = time.time()\n",
    "    #print sim_list\n",
    "    #print 'sim compare time = ', t_stop - t_start, 's'\n",
    "\n",
    "    user_sim_list = []\n",
    "    idx = 0\n",
    "    for i in xrange(user_size):\n",
    "        for j in xrange(user_size):\n",
    "            sim = sim_list[idx]\n",
    "            idx += 1\n",
    "            if str(sim) == '0.0':\n",
    "                continue\n",
    "            user_sim_list.append((user_list[int(i)], user_list[int(j)] + 'ANS', sim))\n",
    "    \n",
    "    from operator import itemgetter \n",
    "    sorted_usl = sorted(user_sim_list, key=itemgetter(2), reverse=True)\n",
    "    def _get_ans_by_sim(sorted_usl, lt_value):\n",
    "        LT_VALUE = lt_value\n",
    "        return sum(sim > LT_VALUE and user_i+'ANS' == user_j for user_i, user_j, sim in sorted_usl)\n",
    "    \n",
    "    def _get_user_by_sim(sorted_usl, lt_value):\n",
    "        LT_VALUE = lt_value\n",
    "        return sum(sim > LT_VALUE for sim in zip(*sorted_usl)[2])\n",
    "    #print 'lt:0.00000', _get_ans_by_sim(sorted_usl, 0) , _get_user_by_sim(sorted_usl, 0)\n",
    "    print 'Num of aw words:', aw\n",
    "    for n in range(1,6):\n",
    "        lt = float('0.'+ str(n))\n",
    "        print 'lt:', lt, _get_ans_by_sim(sorted_usl, lt) , _get_user_by_sim(sorted_usl, lt)\n",
    "    \n",
    "    #name = os.path.splitext(sample_file)[0]\n",
    "    resultpath = loosen_dir + '/' + os.path.splitext(sample_file)[0] + '_cv' + str(wp) + '.txt'\n",
    "    with open(resultpath, 'wb') as fout:\n",
    "        \n",
    "        print 'write to ', resultpath \n",
    "        for user, ans, sim in sorted_usl:\n",
    "            line = user + ',' + ans + ',' + str(sim) +'\\n'\n",
    "            fout.write(line.encode('utf-8'))\n",
    "        fout.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
