{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Trie..., from /home/joekaojoekao/github/dict.txt.big\n",
      "DEBUG:jieba:Building Trie..., from /home/joekaojoekao/github/dict.txt.big\n",
      "loading model from cache /tmp/jieba.user.8815186977980235228.cache\n",
      "DEBUG:jieba:loading model from cache /tmp/jieba.user.8815186977980235228.cache\n",
      "loading model cost 1.74812197685 seconds.\n",
      "DEBUG:jieba:loading model cost 1.74812197685 seconds.\n",
      "Trie has been built succesfully.\n",
      "DEBUG:jieba:Trie has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read source: ./visualized/select_pushes1000_1.txt\n",
      "Pushes filtering (keep only pushes larger than 4 words)\n",
      "After filtering / Origin push num:  999 / 999\n",
      "Users filtering (remove users who post less than 3 pushes)\n",
      "After filtering / Previous user num:  718 / 999\n",
      "write user list: ./temp/select_pushes1000_1_userlist.txt\n",
      "write push data ready to do cross_validation: ./temp/select_pushes1000_1_cvlist.txt\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "%run gen_cv.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read user from  ./temp/select_pushes1000_1_userlist.txt\n",
      "read push from ./temp/select_pushes1000_1_cvlist.txt\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_file = config[\"sample_file\"] #first input\n",
    "temp_path = config[\"temp_path\"] \n",
    "result_dir = config[\"result_dir\"]\n",
    "\n",
    "## just pick some user here from temp folder\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt', 'rb') as f_temp:\n",
    "    print 'read user from ', temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt'\n",
    "    user_list = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt', 'rb') as f_temp:\n",
    "    print 'read push from', temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt'\n",
    "    temp_json = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "    \n",
    "\n",
    "dict_train, dict_test = temp_json # train/test push list for each user\n",
    "\n",
    "print len(dict_train['heavenkghs'])\n",
    "print len(dict_test['heavenkghs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read user from  ./temp/select_pushes1000_1_userlist.txt\n",
      "read push from ./temp/select_pushes1000_1_cvlist.txt\n",
      "total word 9507\n",
      "stop word 249\n",
      "num_word: 1 \t132\n",
      "num_word: 2 \t107\n",
      "num_word: 3 \t7\n",
      "246\n",
      "total word 9507\n",
      "rare word 6073\n",
      "num_word: 1 \t447\n",
      "num_word: 2 \t4048\n",
      "num_word: 3 \t991\n",
      "num_word: 4 \t394\n",
      "num_word: 5 \t50\n",
      "num_word: 6 \t34\n",
      "5964\n",
      "3\n",
      "3\n",
      "718\n",
      "...\n",
      "[ 0.          0.          1.         ...,  0.          0.33333333  1.        ]\n",
      "sim compare time =  0.209052085876 s\n",
      "write to  ./cv_result/select_pushes1000_1_cv.txt\n"
     ]
    }
   ],
   "source": [
    "%%pypy \n",
    "## using pypy magic needs a temp file for output \n",
    "import json\n",
    "import numpypy as np # while at sweslos' centos6.x\n",
    "import time\n",
    "\n",
    "def count_dict(sample_dict_jieba):\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "    import itertools\n",
    "    ptt_pushes_freq_bypush = {}\n",
    "    for uid, push_list in sample_dict_jieba.iteritems():\n",
    "        push = push_list[0] #raw push\n",
    "        a = zip(*push_list)[1] #jieba ones\n",
    "        push_gram = list(itertools.chain(*a))\n",
    "        count = Counter(push_gram)\n",
    "        temp = []\n",
    "        for w, c in count.most_common():\n",
    "            temp.append((w, c))\n",
    "        id_count = Counter(dict(temp))\n",
    "        ptt_pushes_freq_bypush[uid] = dict(id_count)\n",
    "\n",
    "    return ptt_pushes_freq_bypush\n",
    "\n",
    "def weighted_jaccard(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        return -1\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in xrange(len(l1)):\n",
    "        num += np.minimum(l1[i], l2[i])\n",
    "        den += np.maximum(l1[i], l2[i])\n",
    "    wj = np.divide(np.float64(num), den)\n",
    "\n",
    "    return wj\n",
    "\n",
    "import os\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_file = config[\"sample_file\"] #first input\n",
    "temp_path = config[\"temp_path\"] \n",
    "result_dir = config[\"result_dir\"]\n",
    "\n",
    "## just pick some user here from temp folder\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt', 'rb') as f_temp:\n",
    "    print 'read user from ', temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt'\n",
    "    user_list = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt', 'rb') as f_temp:\n",
    "    print 'read push from', temp_path + os.path.splitext(sample_file)[0] + '_cvlist.txt'\n",
    "    temp_json = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "    \n",
    "\n",
    "dict_train, dict_test = temp_json # train/test push list for each user\n",
    "\n",
    "dict_train_count = count_dict(dict_train) #count the freq\n",
    "dict_test_count = count_dict(dict_test) #count the freq for ans(test)\n",
    "\n",
    "from collections import Counter\n",
    "count_all = Counter()\n",
    "for v in dict_train_count.values():\n",
    "    count_all += Counter(v)\n",
    "\n",
    "from collections import OrderedDict\n",
    "sorted_tuple_list_train_count = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "## global term count using train data\n",
    "\n",
    "# with open(temp_path + os.path.splitext(sample_file)[0] + '_train_count.txt', 'wb') as f_temp:\n",
    "#     f_temp.write(json.dumps(dict_train_count, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "#     f_temp.close()\n",
    "\n",
    "################\n",
    "\n",
    "##test function\n",
    "\n",
    "#################Feature Extraction##########################\n",
    "\n",
    "import pttfeat\n",
    "\n",
    "selected_cw = pttfeat.CW(dict_train_count)\n",
    "#print selected_cw\n",
    "\n",
    "selected_aw = pttfeat.AW(dict_train_count)\n",
    "#print selected_aw\n",
    "\n",
    "def user_sen_distrib(user_sl):\n",
    "    \"return distribution of user sentence length\"\n",
    "    LOW = 5\n",
    "    HIGH = 20\n",
    "    sl_stat = {}\n",
    "    for k in user_sl.keys():\n",
    "        user_cut = []\n",
    "        temp = []\n",
    "        for push_sl in user_sl[k]:\n",
    "            push_cut = push_sl['cut']\n",
    "            for s in push_cut:\n",
    "                if s <= LOW:\n",
    "                    temp.append(0)\n",
    "                elif s > LOW and s <= HIGH:\n",
    "                    temp.append(1)\n",
    "                elif s > HIGH:\n",
    "                    temp.append(2) \n",
    "        temp_percent = [float(x) / len(temp) for x in [temp.count(0), temp.count(1), temp.count(2)]]\n",
    "        #print k,temp_percent\n",
    "        sl_stat[k] = temp_percent\n",
    "    return sl_stat\n",
    "\n",
    "user_sl_train = pttfeat.SL(dict_train)\n",
    "sl_stat_train = user_sen_distrib(user_sl_train)\n",
    "\n",
    "user_sl_test = pttfeat.SL(dict_test)\n",
    "sl_stat_test = user_sen_distrib(user_sl_test)\n",
    "\n",
    "    \n",
    "# with open('./feature_pool/' + 'CW.txt', 'wb') as f_feature:\n",
    "#     f_feature.write(json.dumps(selected_cw, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "#     f_feature.close()\n",
    "\n",
    "###############VEC TIME##########################################\n",
    "\n",
    "## 1. gen sw vec\n",
    "# import itertools\n",
    "# general_vec = {}\n",
    "# for uid in user_list: # for each user id\n",
    "#     user_len = sum([len(x) for x in dict_train_count[uid]]) #total word freq\n",
    "#     if user_len > 0:\n",
    "#         vec = [dict_train_count[uid].get(w, 0) for w in selected_cw]\n",
    "#         g_vec = [float(x) / user_len for x in vec]\n",
    "#         general_vec[uid] = g_vec\n",
    "\n",
    "\n",
    "# general_vec_ans = {}\n",
    "# for uid in user_list: # for each user id\n",
    "#     user_len = sum([len(x) for x in dict_test_count[uid]]) #total word freq\n",
    "#     #print sum(v.values())\n",
    "#     if user_len > 0:\n",
    "#         vec = [dict_test_count[uid].get(w, 0) for w in selected_cw]\n",
    "#         g_vec = [float(x) / user_len for x in vec]\n",
    "#         general_vec_ans[uid] = g_vec\n",
    "\n",
    "# print len(general_vec[user_list[0]])\n",
    "# print len(general_vec_ans[user_list[0]])\n",
    "\n",
    "\n",
    "\n",
    "## 2. gen aw vec\n",
    "import itertools\n",
    "general_vec = {}\n",
    "for uid in user_list: # for each user id\n",
    "    user_len = sum([len(x) for x in dict_train_count[uid]]) #total word freq\n",
    "    if user_len > 0:\n",
    "        vec = [dict_train_count[uid].get(w, 0) for w in selected_aw]\n",
    "        g_vec = [float(x) / user_len for x in vec]\n",
    "        general_vec[uid] = g_vec\n",
    "\n",
    "\n",
    "general_vec_ans = {}\n",
    "for uid in user_list: # for each user id\n",
    "    user_len = sum([len(x) for x in dict_test_count[uid]]) #total word freq\n",
    "    #print sum(v.values())\n",
    "    if user_len > 0:\n",
    "        vec = [dict_test_count[uid].get(w, 0) for w in selected_aw]\n",
    "        g_vec = [float(x) / user_len for x in vec]\n",
    "        general_vec_ans[uid] = g_vec\n",
    "\n",
    "print user_list[0]\n",
    "print general_vec[user_list[0]]\n",
    "print general_vec_ans[user_list[0]]\n",
    "\n",
    "\n",
    "## 3. gen sl vec\n",
    "# import itertools\n",
    "# general_vec = {}\n",
    "# for uid in user_list: # for each user id\n",
    "#     #user_len = sum([len(x) for x in dict_train_count[uid]]) #total word freq\n",
    "#     #if user_len > 0:\n",
    "#         #vec = [dict_train_count[uid].get(w, 0) for w in selected_cw]\n",
    "#         #g_vec = [float(x) / user_len for x in vec]\n",
    "#     general_vec[uid] = sl_stat_train[uid]\n",
    "\n",
    "\n",
    "# general_vec_ans = {}\n",
    "# for uid in user_list: # for each user id\n",
    "#     #user_len = sum([len(x) for x in dict_test_count[uid]]) #total word freq\n",
    "#     #print sum(v.values())\n",
    "#     #if user_len > 0:\n",
    "#         #vec = [dict_test_count[uid].get(w, 0) for w in selected_cw]\n",
    "#         #g_vec = [float(x) / user_len for x in vec]\n",
    "#     general_vec_ans[uid] = sl_stat_test[uid]\n",
    "\n",
    "    \n",
    "# print len(general_vec[user_list[0]])\n",
    "# print len(general_vec_ans[user_list[0]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "user_size = len(user_list)\n",
    "print user_size\n",
    "\n",
    "sim_list = np.array(np.arange(user_size*user_size))\n",
    "sim_list = sim_list.astype(float)\n",
    "print '...'\n",
    "idx = 0\n",
    "for i in xrange(user_size):\n",
    "    for j in xrange(user_size):\n",
    "        #wj_sw = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "        #sim_list = np.vstack((sim_list, np.array((round(float(i),1), round(float(j),1), wj_sw))))\n",
    "\n",
    "        sim_list[idx] = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "        idx += 1\n",
    "sim_list = sim_list[:idx]\n",
    "t_stop = time.time()\n",
    "print sim_list\n",
    "print 'sim compare time = ', t_stop - t_start, 's'\n",
    "\n",
    "user_sim_list = []\n",
    "idx = 0\n",
    "for i in xrange(user_size):\n",
    "    for j in xrange(user_size):\n",
    "        sim = sim_list[idx]\n",
    "        idx += 1\n",
    "        user_sim_list.append((user_list[int(i)], user_list[int(j)] + 'ANS', sim))\n",
    "#name = os.path.splitext(sample_file)[0]\n",
    "with open(result_dir + os.path.splitext(sample_file)[0] + '_cv.txt', 'wb') as fout:\n",
    "    resultpath = result_dir + os.path.splitext(sample_file)[0] + '_cv.txt'\n",
    "    print 'write to ', resultpath \n",
    "    for user, ans, sim in user_sim_list:\n",
    "        line = user + ',' + ans + ',' + str(sim) +'\\n'\n",
    "        fout.write(line.encode('utf-8'))\n",
    "    fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j_len_code = []\n",
    "for j in j_len:\n",
    "    temp = []\n",
    "    for s in j:\n",
    "        if s < LOW:\n",
    "            temp.append(0)\n",
    "        elif s >= LOW and s < HIGH:\n",
    "            temp.append(1)\n",
    "        elif s >= HIGH:\n",
    "            temp.append(2)\n",
    "    j_len_code.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u4f60\\u628a\\u4ed6\\u5011\\u7576\\u540c\\u80de', u'\\u4f46\\u4ed6\\u5011\\u7b2c\\u4e00\\u500b\\u51fa\\u8ce3\\u7684\\u5c31\\u662f\\u4f60\\u6642', u'\\u4e0d\\u89ba\\u9192\\u5f88\\u96e3']\n",
      "你把他們當同胞\n",
      "但他們第一個出賣的就是你時\n",
      "不覺醒很難\n"
     ]
    }
   ],
   "source": [
    "%%pypy\n",
    "import pttfeat\n",
    "print pttfeat.cut_sentence(\"你把他們當同胞,但他們第一個出賣的就是你時。不覺醒很難\")\n",
    "a = pttfeat.cut_sentence(\"你把他們當同胞,但他們第一個出賣的就是你時。不覺醒很難\")\n",
    "for aa in a:\n",
    "    print aa.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhon这个Python库提sdf供了常用汉字常sdfs量\n",
      "如CJK字符和偏旁\n",
      "中文标点\n",
      "拼音\n",
      "和汉字正则表达式\n",
      "如找到文本中的繁\n",
      "体字\n"
     ]
    }
   ],
   "source": [
    "PUNCTUATION_TABLE = [\n",
    "    # start,  stop\n",
    "    (0x2000, 0x206f),  # General Punctuation\n",
    "    (0x3000, 0x303f),  # CJK Symbols and Punctuation\n",
    "    (0xff00, 0xffef),  # Halfwidth and Fullwidth Forms\n",
    "    (0x00, 0x2f),  # ascii special charactors\n",
    "    # (0x30, 0x39), # number 0~9\n",
    "    (0x3a, 0x40),  # ascii special charactors\n",
    "    (0x5b, 0x60),  # ascii special charactors\n",
    "    (0x7b, 0xff),  # ascii special charactors\n",
    "]\n",
    "PUNCTUATION_RANGE = reduce(lambda x, y: x.union(y), [\n",
    "                           range(start, stop + 1) for start, stop in PUNCTUATION_TABLE], set())\n",
    "\n",
    "\n",
    "\n",
    "def isPUNC(uchar):\n",
    "    if ord(uchar) in PUNCTUATION_RANGE:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "text = \"Zhon这个Python库提sdf供了常用汉字常sdfs量，如CJK字符和偏旁,.,.,..<><>，中文标点，，，，拼音，和汉字正则表达式（如找到文本中的繁。体字）。：\"\n",
    "#title = re.sub('[%s]' % zhon.hanzi.punctuation, \" \", text)\n",
    "#title = re.sub('[%s]' % sep_list, \" \", title)\n",
    "#print title\n",
    "\n",
    "\n",
    "\n",
    "def cut_sentence(push): ##放入原始文章路徑, 增加斷詞的list\n",
    "    #text = codecs.open(text_path,\"r\",\"utf-8\")   #開檔\n",
    "    PUNCTUATION_TABLE = [\n",
    "    # start,  stop\n",
    "    (0x2000, 0x206f),  # General Punctuation\n",
    "    (0x3000, 0x303f),  # CJK Symbols and Punctuation\n",
    "    (0xff00, 0xffef),  # Halfwidth and Fullwidth Forms\n",
    "    (0x00, 0x2f),  # ascii special charactors\n",
    "    # (0x30, 0x39), # number 0~9\n",
    "    (0x3a, 0x40),  # ascii special charactors\n",
    "    (0x5b, 0x60),  # ascii special charactors\n",
    "    (0x7b, 0xff),  # ascii special charactors\n",
    "    ]\n",
    "    PUNCTUATION_RANGE = reduce(lambda x, y: x.union(y), [\n",
    "                           range(start, stop + 1) for start, stop in PUNCTUATION_TABLE], set())\n",
    "\n",
    "    push = push.decode('utf-8')\n",
    "    sent = \"\"\n",
    "    punt_list = ',.!?:;~，。！？：；～'.decode('utf-8')\n",
    "    cut_list = \"[。，,！……!《》<>\\\"':：？/?、/|“”‘’；]{}（）{}【】()｛｝（）：？！。，;、~——+％%`:“”＂'‘\\n\\r\".decode('utf-8')\n",
    "    import zhon.hanzi # external lib for chinese punc \n",
    "    sep_list = list(set(punt_list + cut_list)) #customized punc list\n",
    "    sent_list = []\n",
    "       \n",
    "    for word in push:\n",
    "        if ord(word) not in PUNCTUATION_RANGE: #如果文字不是標點符號，就把字加到句子中\n",
    "            sent += word\n",
    "            #print sentence\n",
    "        #use re.sub |\n",
    "        #if word not in zhon.hanzi.punctuation:\n",
    "            #sent += word\n",
    "        #if ord(word) not in PUNCTUATION_RANGE:\n",
    "            #sent += word\n",
    "        else:\n",
    "            sentence = sent.strip()\n",
    "            if len(sentence) != 0:\n",
    "                sent_list.append(sentence.strip()) #如果遇到標點符號，把句子加到 text list中\n",
    "            sent = \"\"\n",
    "            #print textList\n",
    "\n",
    "    return sent_list#傳回一個文字陣列\n",
    "for s in cut_sentence(text):\n",
    "    print s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SL(dict_train):\n",
    "    print dict_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FindTok(cutlist, char):\n",
    "    if char in cutlist:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "     \n",
    "def Cut(cutlist,lines):   \n",
    "    l = []   \n",
    "    line = []   \n",
    "        \n",
    "    for i in lines:   \n",
    "        if FindTok(cutlist,i):\n",
    "            line.append(i)\n",
    "            l.append(''.join(line))\n",
    "            #l.append(i)\n",
    "            line = []   \n",
    "        else:   \n",
    "            line.append(i)   \n",
    "    return l  \n",
    " \n",
    "cutlist =\"[。，,！……!《》<>\\\"':：？\\?、\\|“”‘’；]{}（）{}【】()｛｝（）：？！。，;、~——+％%`:“”＂'‘\\n\\r\".decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Push!', u'XDDDDDDDD', u'\\u597d\\u96e3\\u904e \\u4e0d\\u8981\\u6253\\u4ed6\\u5011', u'XDDDD')\n",
      "[u'Push!']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "(u'\\u70ba\\u4ec0\\u9ebc\\u8aaa\\u5be6\\u8a71\\u7684\\u4eba\\u6703\\u88ab\\u4e00\\u7fa4\\u75c5\\u755c\\u5653?', u'\\u70ba\\u4ec0\\u9ebc\\u6c34\\u6876?', u'\\u4e2d\\u56fd\\u4eba\\u4f86\\u9019\\u9178\\u7684\\u771f\\u6c92\\u4eba\\u6027,\\u5168\\u4e0d\\u5f97\\u597d\\u6b7b', u'\\u90fd\\u6709\\u4eba\\u88ab\\u8b66\\u5bdf\\u6253\\u5230\\u6027\\u547d\\u5782\\u5371\\u4e86,\\u90a3\\u4e9b\\u4e2d\\u56fd\\u4eba\\u548c\\u9ee8\\u5de5\\u9084\\u5728\\u9178,\\u6c92\\u826f\\u5fc3', u'\\u52a0\\u6cb9! \\u5e78\\u82e6\\u4e86,\\u795d\\u90a3\\u4e9b\\u4f86\\u5653\\u7684\\u4e2d\\u56fd\\u4eba\\u548c\\u9ee8\\u5de5\\u4e0d\\u5f97\\u597d\\u6b7b', u'\\u99ac\\u82f1\\u4e5d\\u7684\\u7684\\u6b77\\u53f2\\u5b9a\\u4f4d\\u5c31\\u662f\"\\u7368\\u88c1\"', u'\\u63a8! \\u570b\\u6c11\\u9ee8\\u4e00\\u76f4\\u4ee5\\u4f86\\u548c\\u9ed1\\u9053\\u90fd\\u662f\\u540c\\u4e00\\u5925', u'\\u9019\\u5c31\\u662f\\u9ee8\\u5de5\\u554a!')\n",
      "[u'\\u70ba\\u4ec0\\u9ebc\\u8aaa\\u5be6\\u8a71\\u7684\\u4eba\\u6703\\u88ab\\u4e00\\u7fa4\\u75c5\\u755c\\u5653?']\n",
      "[u'\\u70ba\\u4ec0\\u9ebc\\u6c34\\u6876?']\n",
      "[u'\\u4e2d\\u56fd\\u4eba\\u4f86\\u9019\\u9178\\u7684\\u771f\\u6c92\\u4eba\\u6027,']\n",
      "[u'\\u90fd\\u6709\\u4eba\\u88ab\\u8b66\\u5bdf\\u6253\\u5230\\u6027\\u547d\\u5782\\u5371\\u4e86,', u'\\u90a3\\u4e9b\\u4e2d\\u56fd\\u4eba\\u548c\\u9ee8\\u5de5\\u9084\\u5728\\u9178,']\n",
      "[u'\\u52a0\\u6cb9!', u' \\u5e78\\u82e6\\u4e86,']\n",
      "[u'\\u99ac\\u82f1\\u4e5d\\u7684\\u7684\\u6b77\\u53f2\\u5b9a\\u4f4d\\u5c31\\u662f\"', u'\\u7368\\u88c1\"']\n",
      "[u'\\u63a8!']\n",
      "[u'\\u9019\\u5c31\\u662f\\u9ee8\\u5de5\\u554a!']\n",
      "(u'\\u8a71\\u8aaa\\u539f\\u6587\\u5c07\\u81e3\\u7684\\u624b\\u52e2\\u6bd451 \\u6d3b\\u5f97\\u6bd4\\u8f03\\u4e45\\u679c\\u7136\\u6bd4\\u8f03\\u4e0d\\u540cXD',)\n",
      "[]\n",
      "(u'.........',)\n",
      "[]\n",
      "(u'\\u56e0\\u70ba\\u90a3\\u689d\\u68c9\\u88ab\\u662f\\u99ac\\u5361\\u8338\\u84cb\\u4e8634\\u5e74\\u7684\\u68c9\\u88ab \\u5167\\u529b\\u975e\\u51e1\\u963f', u'\\u53f0\\u7063\\u5be6\\u8cea\\u4e0a\\u672c\\u4f86\\u5c31\\u662f\\u7368\\u7acb\\u7684 \\u5dee\\u5225\\u53ea\\u5728\\u570b\\u865f', u'\\u4ee5\\u5f8c\\u53ea\\u8981\\u516c\\u6295\\u90fd\\u8981\\u7acb\\u4e00\\u500b\\u65b0\\u6cd5\\u55ce?', u'\\u4eca\\u5929\\u7279\\u5225\\u689d\\u4f8b\\u901a\\u904e\\u4e86 \\u516c\\u6295\\u6cd5\\u9084\\u662f\\u6c92\\u6539 \\u4e0b\\u6b21\\u9084\\u6709\\u6a5f\\u6703\\u6539\\u55ce?')\n",
      "[]\n",
      "[]\n",
      "[u'\\u4ee5\\u5f8c\\u53ea\\u8981\\u516c\\u6295\\u90fd\\u8981\\u7acb\\u4e00\\u500b\\u65b0\\u6cd5\\u55ce?']\n",
      "[u'\\u4eca\\u5929\\u7279\\u5225\\u689d\\u4f8b\\u901a\\u904e\\u4e86 \\u516c\\u6295\\u6cd5\\u9084\\u662f\\u6c92\\u6539 \\u4e0b\\u6b21\\u9084\\u6709\\u6a5f\\u6703\\u6539\\u55ce?']\n",
      "(u'25...    \\u61c9\\u8a72\\u5f88\\u4e45\\u4e0d\\u6703\\u8cb750\\u5d50\\u4e86XD',)\n",
      "[]\n",
      "(u'\\u53f0\\u7063\\u91ab\\u7642\\u548c\\u6559\\u80b2\\u7684\\u554f\\u984c\\u8d8a\\u4f86\\u8d8a\\u5927...\\u9019\\u662f\\u6703\\u52d5\\u6416\\u570b\\u672c\\u7684 ...',)\n",
      "[]\n",
      "(u'\\u63a8 \\u8ab0\\u5feb\\u8cbc\\u5728\\u99ac\\u4e45\\u7684\\u81c9\\u4e0a', u'\\u63a8 \\u8ab0\\u5feb\\u8cbc\\u5728\\u99ac\\u4e45\\u7684\\u81c9\\u4e0a')\n",
      "[]\n",
      "[]\n",
      "(u'\\u9084\\u6709\\u4e0a\\u6b21\\u7684\\u982d\\u7248\\uff0c\\u7528QRcode\\u771f\\u7684\\u6703\\u7279\\u5225\\u53bb\\u6383\\u4f86\\u770b\\u7684\\u53c8\\u6709', u'\\u6c92\\u670900...')\n",
      "[u'\\u9084\\u6709\\u4e0a\\u6b21\\u7684\\u982d\\u7248\\uff0c']\n",
      "[]\n",
      "(u'\\u4e0a\\u73ed\\u7e41\\u5fd9, \\u4f46\\u6709\\u591a\\u5c11\\u5c31\\u7ffb\\u591a\\u5c11. \\u6587\\u592a\\u9577\\u6216\\u662f\\u6c92\\u6642\\u9593\\u7ffb\\u7684\\u8a71', u'\\u6709\\u5be6\\u969b\\u8ddf\\u4e2d\\u570b\\u4eba\\u5171\\u4e8b\\u53c8\\u6c92\\u6709\\u5728\\u4e2d\\u570b\\u6709\\u5229\\u76ca\\u95dc\\u4fc2,\\u8981\\u9192\\u904e\\u4f86\\u4e0d\\u96e3')\n",
      "[u'\\u4e0a\\u73ed\\u7e41\\u5fd9,']\n",
      "[u'\\u6709\\u5be6\\u969b\\u8ddf\\u4e2d\\u570b\\u4eba\\u5171\\u4e8b\\u53c8\\u6c92\\u6709\\u5728\\u4e2d\\u570b\\u6709\\u5229\\u76ca\\u95dc\\u4fc2,']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# def cut_sentence_new(words):\n",
    "#     # words = (words).decode('utf8')\n",
    "#     start = 0\n",
    "#     i = 0\n",
    "#     sents = []\n",
    "#     punt_list = ',.!?:;~，。！？：；～'.decode('utf8')\n",
    "#     cutlist = \"[。，,！……!《》<>\\\"':：？/?、/|“”‘’；]{}（）{}【】()｛｝（）：？！。，;、~——+％%`:“”＂'‘\\n\\r\".decode('utf-8')\n",
    "#     sep_list = list(set(punt_list + cut_list))\n",
    "#     for word in words:\n",
    "#         if word in sep_list and token not in sep_list: #检查标点符号下一个字符是否还是标点\n",
    "#             sents.append(words[start:i+1])\n",
    "#             start = i+1\n",
    "#             i += 1\n",
    "#         else:\n",
    "#             i += 1\n",
    "#             token = list(words[start:i+2]).pop() # 取下一个字符\n",
    "#     if start < len(words):\n",
    "#         sents.append(words[start:])\n",
    "#     return sents\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist_new.txt', 'rb') as f_temp:\n",
    "    temp_json = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "#print temp\n",
    "sample_dict_jieba = temp_json[0]\n",
    "import itertools\n",
    "ptt_pushes_freq_bypush = {}\n",
    "j = {}\n",
    "c = 0\n",
    "for uid, push_list in sample_dict_jieba.iteritems():\n",
    "    #push = push_list[0] #raw push\n",
    "    pushes = zip(*push_list)[0] #jieba ones\n",
    "    #print a\n",
    "    #print cut_sentence_new(a)\n",
    "    print pushes\n",
    "    for push in pushes:\n",
    "        l = Cut(list(cutlist),list(push))\n",
    "        print l\n",
    "#     j[uid] = []\n",
    "    \n",
    "#     for line in l:\n",
    "#         if line.strip() != \"\":\n",
    "#             li = line.strip().split()\n",
    "#             for sent in li:\n",
    "#                 print sent\n",
    "#         #j[uid].append()\n",
    "    c+=1\n",
    "    if c == 10:\n",
    "        break\n",
    "#print json.dumps(j)\n",
    "\n",
    "# for lines in file(\"t.txt\"):    \n",
    "#     l = Cut(list(cutlist),list(lines.decode('gbk')))     \n",
    "#     for line in l:  \n",
    "#        if line.strip() !=\"\":      \n",
    "#             li = line.strip().split()   \n",
    "#             for sentence in li:\n",
    "#                 print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "64\n",
      "48\n",
      "[u'\\uff01', u'\\u3001', u'\\u3002', u'\\uff05', u'\\uff09', u'\\uff08', u'\\u300b', u'\\u300a', u'\\r', u'\\uff0c', u'\\u3011', u'\\u3010', u'\\uff02', u'\\u2014', u'\\u2019', u'\\u2018', u'\\uff1b', u'\\uff1a', u'\\u201d', u'\\u201c', u'\\uff1f', u'!', u'\"', u'%', u\"'\", u'\\u2026', u')', u'(', u'+', u',', u'/', u'.', u';', u':', u'<', u'?', u'>', u'\\n', u'\\uff5b', u'[', u']', u'\\uff5e', u'`', u'\\uff5d', u'{', u'}', u'|', u'~']\n"
     ]
    }
   ],
   "source": [
    "punt_list = ',.!?:;~，。！？：；～'.decode('utf8')\n",
    "cut_list = \"[。，,！……!《》<>\\\"':：？/?、/|“”‘’；]{}（）{}【】()｛｝（）：？！。，;、~——+％%`:“”＂'‘\\n\\r\".decode('utf-8')\n",
    "print len(punt_list)\n",
    "print len(cut_list)\n",
    "sep_list = list(set(punt_list + cut_list))\n",
    "print len(sep_list)\n",
    "print sep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION_TABLE = [\n",
    "    # start,  stop\n",
    "    (0x2000, 0x206f),  # General Punctuation\n",
    "    (0x3000, 0x303f),  # CJK Symbols and Punctuation\n",
    "    (0xff00, 0xffef),  # Halfwidth and Fullwidth Forms\n",
    "    (0x00, 0x2f),  # ascii special charactors\n",
    "    # (0x30, 0x39), # number 0~9\n",
    "    (0x3a, 0x40),  # ascii special charactors\n",
    "    (0x5b, 0x60),  # ascii special charactors\n",
    "    (0x7b, 0xff),  # ascii special charactors\n",
    "]\n",
    "PUNCTUATION_RANGE = reduce(lambda x, y: x.union(y), [\n",
    "                           range(start, stop + 1) for start, stop in PUNCTUATION_TABLE], set())\n",
    "\n",
    "\n",
    "\n",
    "def isPUNC(uchar):\n",
    "    if ord(uchar) in PUNCTUATION_RANGE:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se: 漢皇重色思傾國，\n",
      "se: 御宇多年求不得。\n",
      "se: 楊家有女初長成，\n",
      "se: 養在深閨人未識。\n",
      "se: 天生麗質難自棄，\n",
      "se: 一朝選在君王側。\n",
      "se: 回眸一笑百媚生，\n",
      "se: 六宮粉黛無顏色。\n",
      "se: 春寒賜浴華清池，\n",
      "se: 溫泉水滑洗凝脂。\n",
      "se: 侍兒扶起嬌無力，\n",
      "se: 始是新承恩澤時。\n",
      "se: 雲鬢花顏金步搖，\n",
      "se: 芙蓉帳暖度春宵。\n",
      "se: 春宵苦短日高起，\n",
      "se: 從此君王不早朝。\n",
      "se: 承歡侍宴無閒暇，\n",
      "se: 春從春遊夜專夜。\n",
      "se: 後宮佳麗三千人，\n",
      "se: 三千寵愛在一身。\n",
      "se: 金屋妝成嬌侍夜，\n",
      "se: 玉樓宴罷醉和春。\n",
      "se: 姊妹兄弟皆列土，\n",
      "se: 可憐光彩生門戶。\n",
      "se: 遂令天下父母心，\n",
      "se: 不重生男重生女。\n",
      "se: 驪宮高處入青雲，\n",
      "se: 仙樂風飄處處聞。\n",
      "se: 緩歌慢舞凝絲竹，\n",
      "se: 盡日君王看不足。\n",
      "se: 漁陽鼙鼓動地來，\n",
      "se: 驚破霓裳羽衣曲。\n",
      "se: 九重城闕煙塵生，\n",
      "se: 千乘萬騎西南行。\n",
      "se: 翠華颻颻行復止，\n",
      "se: 西出都門百餘里。\n",
      "se: 六軍不發無奈何，\n",
      "se: 宛轉蛾眉馬前死。\n",
      "se: 花鈿委地無人收，\n",
      "se: 翠翹金雀玉搔頭。\n",
      "se: 君王掩面救不得，\n",
      "se: 回看血淚相和流。\n",
      "se: 黃埃散漫風蕭索，\n",
      "se: 雲棧縈紆登劍閣。\n",
      "se: 峨嵋山下少人行，\n",
      "se: 旌旗無光日色薄。\n",
      "se: 蜀江水碧蜀山青，\n",
      "se: 聖主朝朝暮暮情。\n",
      "se: 行宮見月傷心色，\n",
      "se: 夜雨聞鈴腸斷聲。\n",
      "se: 天旋日轉回龍馭，\n",
      "se: 到此躊躇不能去。\n",
      "se: 馬嵬坡下泥土中，\n",
      "se: 不見玉顏空死處。\n",
      "se: 君臣相顧盡霑衣，\n",
      "se: 東望都門信馬歸。\n",
      "se: 歸來池苑皆依舊，\n",
      "se: 太液芙蓉未央柳。\n",
      "se: 芙蓉如面柳如眉，\n",
      "se: 對此如何不淚垂。\n",
      "se: 春風桃李花開日，\n",
      "se: 秋雨梧桐葉落時。\n",
      "se: 西宮南內多秋草，\n",
      "se: 落葉滿階紅不掃。\n",
      "se: 梨園弟子白髮新，\n",
      "se: 椒房阿監青娥老。\n",
      "se: 夕殿螢飛思悄然，\n",
      "se: 孤燈挑盡未成眠。\n",
      "se: 遲遲鐘鼓初長夜，\n",
      "se: 耿耿星河欲曙天。\n",
      "se: 鴛鴦瓦冷霜華重，\n",
      "se: 翡翠衾寒誰與共。\n",
      "se: 悠悠生死別經年，\n",
      "se: 魂魄不曾來入夢。\n",
      "se: 臨邛道士鴻都客，\n",
      "se: 能以精誠致魂魄。\n",
      "se: 為感君王輾轉思，\n",
      "se: 遂教方士殷勤覓。\n",
      "se: 排雲馭氣奔如電，\n",
      "se: 昇天入地求之遍。\n",
      "se: 上窮碧落下黃泉，\n",
      "se: 兩處茫茫皆不見。\n",
      "se: 忽聞海上有仙山，\n",
      "se: 山在虛無縹緲間。\n",
      "se: 樓閣玲瓏五雲起，\n",
      "se: 其中綽約多仙子。\n",
      "se: 中有一人字太真，\n",
      "se: 雪膚花貌參差是。\n",
      "se: 金闕西廂叩玉扃，\n",
      "se: 轉教小玉報雙成。\n",
      "se: 聞道漢家天子使，\n",
      "se: 九華帳裏夢魂驚。\n",
      "se: 攬衣推枕起徘徊，\n",
      "se: 珠箔銀屏迤邐開。\n",
      "se: 雲髻半偏新睡覺，\n",
      "se: 花冠不整下堂來。\n",
      "se: 風吹仙袂飄颻舉，\n",
      "se: 猶似霓裳羽衣舞。\n",
      "se: 玉容寂寞淚闌干，\n",
      "se: 梨花一枝春帶雨。\n",
      "se: 含情凝睇謝君王，\n",
      "se: 一別音容兩渺茫。\n",
      "se: 昭陽殿裏恩愛絕，\n",
      "se: 蓬萊宮中日月長。\n",
      "se: 回頭下望人寰處，\n",
      "se: 不見長安見塵霧。\n",
      "se: 唯將舊物表深情，\n",
      "se: 鈿合金釵寄將去。\n",
      "se: 釵留一股合一扇，\n",
      "se: 釵擘黃金合分鈿。\n",
      "se: 但教心似金鈿堅.....,\n",
      "se: ,\n",
      "se: )\n",
      "se: (\n",
      "se: ，\n",
      "se: 天上人間會相見。\n",
      "se: ..\n",
      "se: 臨別殷勤重寄詞，\n",
      "se: 詞中有誓兩心知。\n",
      "se: 七月七日長生殿，\n",
      "se: 夜半無人私語時。\n",
      "se: 在天願作比翼鳥，\n",
      "se: 在地願為連理枝。\n",
      "se: 天長地久有時盡，\n",
      "se: <\n",
      "se: >\n",
      "se: <\n",
      "se: >\n",
      "se: ,\n",
      "se: .,\n",
      "se: .,\n",
      "se: .,\n",
      "se: ,\n",
      "se: ,\n",
      "se: .\n",
      "se: 此恨綿綿無絕期。\n"
     ]
    }
   ],
   "source": [
    "def FindTok(cutlist, char):\n",
    "    if char in cutlist:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "     \n",
    "def Cut(cutlist,lines):   \n",
    "    l = []   \n",
    "    line = []   \n",
    "        \n",
    "    for i in lines:   \n",
    "        if FindTok(cutlist,i):\n",
    "            line.append(i)\n",
    "            l.append(''.join(line))\n",
    "            #l.append(i)\n",
    "            line = []   \n",
    "        else:   \n",
    "            line.append(i)   \n",
    "    return l  \n",
    "#from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "cutlist = \"[。，,！……!《》<>\\\"':：？/?、/|“”‘’；]{}（）{}【】()｛｝（）：？！。，;、~——+％%`:“”＂'‘\\n\\r\".decode('utf-8') \n",
    "for lines in file(\"testfile.txt\"):  \n",
    "    l = Cut(list(cutlist),list(lines.decode('utf-8')))  \n",
    "    for line in l:  \n",
    "       if line.strip() <> \"\":#这里可能包含空格  \n",
    "            li = line.strip().split()  \n",
    "            for sentence in li:  \n",
    "                print \"se:\",sentence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se: 漢皇重色思傾國，\n",
      "se: 御宇多年求不得。\n",
      "se: 楊家有女初長成，\n",
      "se: 養在深閨人未識。\n",
      "se: 天生麗質難自棄，\n",
      "se: 一朝選在君王側。\n",
      "se: 回眸一笑百媚生，\n",
      "se: 六宮粉黛無顏色。\n",
      "se: 春寒賜浴華清池，\n",
      "se: 溫泉水滑洗凝脂。\n",
      "se: 侍兒扶起嬌無力，\n",
      "se: 始是新承恩澤時。\n",
      "se: 雲鬢花顏金步搖，\n",
      "se: 芙蓉帳暖度春宵。\n",
      "se: 春宵苦短日高起，\n",
      "se: 從此君王不早朝。\n",
      "se: 承歡侍宴無閒暇，\n",
      "se: 春從春遊夜專夜。\n",
      "se: 後宮佳麗三千人，\n",
      "se: 三千寵愛在一身。\n",
      "se: 金屋妝成嬌侍夜，\n",
      "se: 玉樓宴罷醉和春。\n",
      "se: 姊妹兄弟皆列土，\n",
      "se: 可憐光彩生門戶。\n",
      "se: 遂令天下父母心，\n",
      "se: 不重生男重生女。\n",
      "se: 驪宮高處入青雲，\n",
      "se: 仙樂風飄處處聞。\n",
      "se: 緩歌慢舞凝絲竹，\n",
      "se: 盡日君王看不足。\n",
      "se: 漁陽鼙鼓動地來，\n",
      "se: 驚破霓裳羽衣曲。\n",
      "se: 九重城闕煙塵生，\n",
      "se: 千乘萬騎西南行。\n",
      "se: 翠華颻颻行復止，\n",
      "se: 西出都門百餘里。\n",
      "se: 六軍不發無奈何，\n",
      "se: 宛轉蛾眉馬前死。\n",
      "se: 花鈿委地無人收，\n",
      "se: 翠翹金雀玉搔頭。\n",
      "se: 君王掩面救不得，\n",
      "se: 回看血淚相和流。\n",
      "se: 黃埃散漫風蕭索，\n",
      "se: 雲棧縈紆登劍閣。\n",
      "se: 峨嵋山下少人行，\n",
      "se: 旌旗無光日色薄。\n",
      "se: 蜀江水碧蜀山青，\n",
      "se: 聖主朝朝暮暮情。\n",
      "se: 行宮見月傷心色，\n",
      "se: 夜雨聞鈴腸斷聲。\n",
      "se: 天旋日轉回龍馭，\n",
      "se: 到此躊躇不能去。\n",
      "se: 馬嵬坡下泥土中，\n",
      "se: 不見玉顏空死處。\n",
      "se: 君臣相顧盡霑衣，\n",
      "se: 東望都門信馬歸。\n",
      "se: 歸來池苑皆依舊，\n",
      "se: 太液芙蓉未央柳。\n",
      "se: 芙蓉如面柳如眉，\n",
      "se: 對此如何不淚垂。\n",
      "se: 春風桃李花開日，\n",
      "se: 秋雨梧桐葉落時。\n",
      "se: 西宮南內多秋草，\n",
      "se: 落葉滿階紅不掃。\n",
      "se: 梨園弟子白髮新，\n",
      "se: 椒房阿監青娥老。\n",
      "se: 夕殿螢飛思悄然，\n",
      "se: 孤燈挑盡未成眠。\n",
      "se: 遲遲鐘鼓初長夜，\n",
      "se: 耿耿星河欲曙天。\n",
      "se: 鴛鴦瓦冷霜華重，\n",
      "se: 翡翠衾寒誰與共。\n",
      "se: 悠悠生死別經年，\n",
      "se: 魂魄不曾來入夢。\n",
      "se: 臨邛道士鴻都客，\n",
      "se: 能以精誠致魂魄。\n",
      "se: 為感君王輾轉思，\n",
      "se: 遂教方士殷勤覓。\n",
      "se: 排雲馭氣奔如電，\n",
      "se: 昇天入地求之遍。\n",
      "se: 上窮碧落下黃泉，\n",
      "se: 兩處茫茫皆不見。\n",
      "se: 忽聞海上有仙山，\n",
      "se: 山在虛無縹緲間。\n",
      "se: 樓閣玲瓏五雲起，\n",
      "se: 其中綽約多仙子。\n",
      "se: 中有一人字太真，\n",
      "se: 雪膚花貌參差是。\n",
      "se: 金闕西廂叩玉扃，\n",
      "se: 轉教小玉報雙成。\n",
      "se: 聞道漢家天子使，\n",
      "se: 九華帳裏夢魂驚。\n",
      "se: 攬衣推枕起徘徊，\n",
      "se: 珠箔銀屏迤邐開。\n",
      "se: 雲髻半偏新睡覺，\n",
      "se: 花冠不整下堂來。\n",
      "se: 風吹仙袂飄颻舉，\n",
      "se: 猶似霓裳羽衣舞。\n",
      "se: 玉容寂寞淚闌干，\n",
      "se: 梨花一枝春帶雨。\n",
      "se: 含情凝睇謝君王，\n",
      "se: 一別音容兩渺茫。\n",
      "se: 昭陽殿裏恩愛絕，\n",
      "se: 蓬萊宮中日月長。\n",
      "se: 回頭下望人寰處，\n",
      "se: 不見長安見塵霧。\n",
      "se: 唯將舊物表深情，\n",
      "se: 鈿合金釵寄將去。\n",
      "se: 釵留一股合一扇，\n",
      "se: 釵擘黃金合分鈿。\n",
      "se: 但教心似金鈿堅.....,,)(，\n",
      "se: 天上人間會相見。\n",
      "se: ..\n",
      "se: 臨別殷勤重寄詞，\n",
      "se: 詞中有誓兩心知。\n",
      "se: 七月七日長生殿，\n",
      "se: 夜半無人私語時。\n",
      "se: 在天願作比翼鳥，\n",
      "se: 在地願為連理枝。\n",
      "se: 天長地久有時盡，<><>,\n",
      "se: .,\n",
      "se: .,\n",
      "se: .,,,\n",
      "se: .\t　此恨綿綿無絕期。\n"
     ]
    }
   ],
   "source": [
    "def cut_sentence_new(words):\n",
    "    # words = (words).decode('utf8')\n",
    "    start = 0\n",
    "    i = 0\n",
    "    sents = []\n",
    "    cutlist = \"[。，,！……!《》<>\\\"':：？/?、/|“”‘’；]{}（）{}【】()｛｝（）：？！。，;、~——+％%`:“”＂'‘\\n\\r\".decode('utf-8')\n",
    "    punt_list = cutlist\n",
    "    for word in words:\n",
    "        if word in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点\n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            token = list(words[start:i+2]).pop() # 取下一个字符\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents\n",
    "f = open(\"testfile.txt\",'rb')\n",
    "for s in cut_sentence_new(f.read().decode('utf-8')):\n",
    "    print 'se:',s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "segmenter = StanfordSegmenter(path_to_jar='./stanford-segmenter/stanford-segmenter-3.4.1.jar', path_to_sihan_corpora_dict='./stanford-segmenter/data', path_to_model='./stanford-segmenter/data/pku.gz', path_to_dict='./stanford-segmenter/data/dict-chris6.ser.gz')\n",
    "dir(segmenter)\n",
    "for s in segmenter.tokenize_sents(f.read().decode('utf-8')):\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "漢皇重色思傾國，\t　御宇多年求不得。\n",
      "楊家有女初長成，\t　養在深閨人未識。\n",
      "天生麗質難自棄，\t　一朝選在君王側。\n",
      "回眸一笑百媚生，\t　六宮粉黛無顏色。\n",
      "春寒賜浴華清池，\t　溫泉水滑洗凝脂。\n",
      "侍兒扶起嬌無力，\t　始是新承恩澤時。\n",
      "雲鬢花顏金步搖，\t　芙蓉帳暖度春宵。\n",
      "春宵苦短日高起，\t　從此君王不早朝。\n",
      "承歡侍宴無閒暇，\t　春從春遊夜專夜。\n",
      "後宮佳麗三千人，\t　三千寵愛在一身。\n",
      "金屋妝成嬌侍夜，\t　玉樓宴罷醉和春。\n",
      "姊妹兄弟皆列土，\t　可憐光彩生門戶。\n",
      "遂令天下父母心，\t　不重生男重生女。\n",
      "驪宮高處入青雲，\t　仙樂風飄處處聞。\n",
      "緩歌慢舞凝絲竹，\t　盡日君王看不足。\n",
      "漁陽鼙鼓動地來，\t　驚破霓裳羽衣曲。\n",
      "九重城闕煙塵生，\t　千乘萬騎西南行。\n",
      "翠華颻颻行復止，\t　西出都門百餘里。\n",
      "六軍不發無奈何，\t　宛轉蛾眉馬前死。\n",
      "花鈿委地無人收，\t　翠翹金雀玉搔頭。\n",
      "君王掩面救不得，\t　回看血淚相和流。\n",
      "黃埃散漫風蕭索，\t　雲棧縈紆登劍閣。\n",
      "峨嵋山下少人行，\t　旌旗無光日色薄。\n",
      "蜀江水碧蜀山青，\t　聖主朝朝暮暮情。\n",
      "行宮見月傷心色，\t　夜雨聞鈴腸斷聲。\n",
      "天旋日轉回龍馭，\t　到此躊躇不能去。\n",
      "馬嵬坡下泥土中，\t　不見玉顏空死處。\n",
      "君臣相顧盡霑衣，\t　東望都門信馬歸。\n",
      "歸來池苑皆依舊，\t　太液芙蓉未央柳。\n",
      "芙蓉如面柳如眉，\t　對此如何不淚垂。\n",
      "春風桃李花開日，\t　秋雨梧桐葉落時。\n",
      "西宮南內多秋草，\t　落葉滿階紅不掃。\n",
      "梨園弟子白髮新，\t　椒房阿監青娥老。\n",
      "夕殿螢飛思悄然，\t　孤燈挑盡未成眠。\n",
      "遲遲鐘鼓初長夜，\t　耿耿星河欲曙天。\n",
      "鴛鴦瓦冷霜華重，\t　翡翠衾寒誰與共。\n",
      "悠悠生死別經年，\t　魂魄不曾來入夢。\n",
      "臨邛道士鴻都客，\t　能以精誠致魂魄。\n",
      "為感君王輾轉思，\t　遂教方士殷勤覓。\n",
      "排雲馭氣奔如電，\t　昇天入地求之遍。\n",
      "上窮碧落下黃泉，\t　兩處茫茫皆不見。\n",
      "忽聞海上有仙山，\t　山在虛無縹緲間。\n",
      "樓閣玲瓏五雲起，\t　其中綽約多仙子。\n",
      "中有一人字太真，\t　雪膚花貌參差是。\n",
      "金闕西廂叩玉扃，\t　轉教小玉報雙成。\n",
      "聞道漢家天子使，\t　九華帳裏夢魂驚。\n",
      "攬衣推枕起徘徊，\t　珠箔銀屏迤邐開。\n",
      "雲髻半偏新睡覺，\t　花冠不整下堂來。\n",
      "風吹仙袂飄颻舉，\t　猶似霓裳羽衣舞。\n",
      "玉容寂寞淚闌干，\t　梨花一枝春帶雨。\n",
      "含情凝睇謝君王，\t　一別音容兩渺茫。\n",
      "昭陽殿裏恩愛絕，\t　蓬萊宮中日月長。\n",
      "回頭下望人寰處，\t　不見長安見塵霧。\n",
      "唯將舊物表深情，\t　鈿合金釵寄將去。\n",
      "釵留一股合一扇，\t　釵擘黃金合分鈿。\n",
      "但教心似金鈿堅.....,,)(，\t　天上人間會相見。..\n",
      "臨別殷勤重寄詞，\t　詞中有誓兩心知。\n",
      "七月七日長生殿，\t　夜半無人私語時。\n",
      "在天願作比翼鳥，\t　在地願為連理枝。\n",
      "天長地久有時盡，<><>,.,.,.,,,.\n",
      "-----\n",
      "此恨綿綿無絕期。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/chinese.pickle')\n",
    "fp = open(\"testfile.txt\")\n",
    "data = fp.read().decode('utf-8')\n",
    "print '\\n-----\\n'.join(tokenizer.tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se: 漢皇重色思傾國\n",
      "se: 御宇多年求不得\n",
      "se: 楊家有女初長成\n",
      "se: 養在深閨人未識\n",
      "se: 天生麗質難自棄\n",
      "se: 一朝選在君王側\n",
      "se: 回眸一笑百媚生\n",
      "se: 六宮粉黛無顏色\n",
      "se: 春寒賜浴華清池\n",
      "se: 溫泉水滑洗凝脂\n",
      "se: 侍兒扶起嬌無力\n",
      "se: 始是新承恩澤時\n",
      "se: 雲鬢花顏金步搖\n",
      "se: 芙蓉帳暖度春宵\n",
      "se: 春宵苦短日高起\n",
      "se: 從此君王不早朝\n",
      "se: 承歡侍宴無閒暇\n",
      "se: 春從春遊夜專夜\n",
      "se: 後宮佳麗三千人\n",
      "se: 三千寵愛在一身\n",
      "se: 金屋妝成嬌侍夜\n",
      "se: 玉樓宴罷醉和春\n",
      "se: 姊妹兄弟皆列土\n",
      "se: 可憐光彩生門戶\n",
      "se: 遂令天下父母心\n",
      "se: 不重生男重生女\n",
      "se: 驪宮高處入青雲\n",
      "se: 仙樂風飄處處聞\n",
      "se: 緩歌慢舞凝絲竹\n",
      "se: 盡日君王看不足\n",
      "se: 漁陽鼙鼓動地來\n",
      "se: 驚破霓裳羽衣曲\n",
      "se: 九重城闕煙塵生\n",
      "se: 千乘萬騎西南行\n",
      "se: 翠華颻颻行復止\n",
      "se: 西出都門百餘里\n",
      "se: 六軍不發無奈何\n",
      "se: 宛轉蛾眉馬前死\n",
      "se: 花鈿委地無人收\n",
      "se: 翠翹金雀玉搔頭\n",
      "se: 君王掩面救不得\n",
      "se: 回看血淚相和流\n",
      "se: 黃埃散漫風蕭索\n",
      "se: 雲棧縈紆登劍閣\n",
      "se: 峨嵋山下少人行\n",
      "se: 旌旗無光日色薄\n",
      "se: 蜀江水碧蜀山青\n",
      "se: 聖主朝朝暮暮情\n",
      "se: 行宮見月傷心色\n",
      "se: 夜雨聞鈴腸斷聲\n",
      "se: 天旋日轉回龍馭\n",
      "se: 到此躊躇不能去\n",
      "se: 馬嵬坡下泥土中\n",
      "se: 不見玉顏空死處\n",
      "se: 君臣相顧盡霑衣\n",
      "se: 東望都門信馬歸\n",
      "se: 歸來池苑皆依舊\n",
      "se: 太液芙蓉未央柳\n",
      "se: 芙蓉如面柳如眉\n",
      "se: 對此如何不淚垂\n",
      "se: 春風桃李花開日\n",
      "se: 秋雨梧桐葉落時\n",
      "se: 西宮南內多秋草\n",
      "se: 落葉滿階紅不掃\n",
      "se: 梨園弟子白髮新\n",
      "se: 椒房阿監青娥老\n",
      "se: 夕殿螢飛思悄然\n",
      "se: 孤燈挑盡未成眠\n",
      "se: 遲遲鐘鼓初長夜\n",
      "se: 耿耿星河欲曙天\n",
      "se: 鴛鴦瓦冷霜華重\n",
      "se: 翡翠衾寒誰與共\n",
      "se: 悠悠生死別經年\n",
      "se: 魂魄不曾來入夢\n",
      "se: 臨邛道士鴻都客\n",
      "se: 能以精誠致魂魄\n",
      "se: 為感君王輾轉思\n",
      "se: 遂教方士殷勤覓\n",
      "se: 排雲馭氣奔如電\n",
      "se: 昇天入地求之遍\n",
      "se: 上窮碧落下黃泉\n",
      "se: 兩處茫茫皆不見\n",
      "se: 忽聞海上有仙山\n",
      "se: 山在虛無縹緲間\n",
      "se: 樓閣玲瓏五雲起\n",
      "se: 其中綽約多仙子\n",
      "se: 中有一人字太真\n",
      "se: 雪膚花貌參差是\n",
      "se: 金闕西廂叩玉扃\n",
      "se: 轉教小玉報雙成\n",
      "se: 聞道漢家天子使\n",
      "se: 九華帳裏夢魂驚\n",
      "se: 攬衣推枕起徘徊\n",
      "se: 珠箔銀屏迤邐開\n",
      "se: 雲髻半偏新睡覺\n",
      "se: 花冠不整下堂來\n",
      "se: 風吹仙袂飄颻舉\n",
      "se: 猶似霓裳羽衣舞\n",
      "se: 玉容寂寞淚闌干\n",
      "se: 梨花一枝春帶雨\n",
      "se: 含情凝睇謝君王\n",
      "se: 一別音容兩渺茫\n",
      "se: 昭陽殿裏恩愛絕\n",
      "se: 蓬萊宮中日月長\n",
      "se: 回頭下望人寰處\n",
      "se: 不見長安見塵霧\n",
      "se: 唯將舊物表深情\n",
      "se: 鈿合金釵寄將去\n",
      "se: 釵留一股合一扇\n",
      "se: 釵擘黃金合分鈿\n",
      "se: 但教心似金鈿堅\n",
      "se: 天上人間會相見\n",
      "se: 臨別殷勤重寄詞\n",
      "se: 詞中有誓兩心知\n",
      "se: 七月七日長生殿\n",
      "se: 夜半無人私語時\n",
      "se: 在天願作比翼鳥\n",
      "se: 在地願為連理枝\n",
      "se: 天長地久有時盡\n",
      "se: 此恨綿綿無絕期\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "#處理編碼的套件\n",
    "import operator\n",
    "##處理字典檔排序的套件\n",
    "cut_list = \"<>/:：;；,、＂’，.。！？｢\\\"\\'\\\\\\n\\r《》“”!@#$%^&*()\".decode(\"utf-8\")  ##列出標點符號，並轉換成utf-8的格式 \n",
    "\n",
    "def cutSentence(text_path, keywords): ##放入原始文章路徑, 增加斷詞的list\n",
    "    text = codecs.open(text_path,\"r\",\"utf-8\")   #開檔\n",
    "    sent = \"\"\n",
    "    textList = []\n",
    "       \n",
    "    for line in text.readlines():\n",
    "        line = line.strip() ##清除空白\n",
    "        \n",
    "        for keyword in keywords:  #清除關鍵字\n",
    "            line = \"\".join(line.split(keyword))\n",
    "            \n",
    "        for word in line:\n",
    "            if word not in sep_list: #如果文字不是標點符號，就把字加到句子中\n",
    "                sent += word\n",
    "                #print sentence\n",
    "            else:\n",
    "                sentence = sent.strip()\n",
    "                if len(sentence) != 0:\n",
    "                    textList.append(sentence.strip()) #如果遇到標點符號，把句子加到 text list中\n",
    "                sent = \"\"\n",
    "                #print textList\n",
    "\n",
    "    return textList#傳回一個文字陣列\n",
    "\n",
    "for s in cutSentence(\"testfile.txt\", \"\"):\n",
    "    print 'se:',s\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
