{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u' ', u'!', u'\\u7684', u'\\u4e86', u'\\uff0c', u'\\u662f', u'~', u'?', u'=', u'\\u6211', u'\\u6709', u'\\u5c31', u'\\u4f60', u'\\u90fd', u'\\uff01', u'\\u8981', u'\\u4e5f', u'\\u4e0d', u'\\u55ce', u'\\u63a8', u'\\u5728', u'\\u4eba', u'\\u88ab', u'\\u597d', u'\\u4ed6', u'\\u8aaa', u'\\u963f', u'\\u5427', u'\\u554a', u'\\u6703', u'\\uff1f', u'\\u770b', u'\\u5f88', u'\\u6c92', u',', u'\\u5566', u'\\u9019', u'\\u53bb', u'\\u8ddf', u'/', u'\"', u'-', u'\\u5594', u'\\u70ba', u'\\u5e79', u'\\u3002', u'\\u53c8', u'(', u')', u'\\u5230', u'\\u624d', u'\\u8b93', u'\\u90a3', u'\\u60f3', u'.', u'\\u99ac', u'\\u4f46', u'\\u5c0d', u'\\u518d', u'\\uffe3', u'\\u628a', u'\\u8ab0', u':', u'\\u80fd', u'\\u591a', u'\\u4e0a', u'\\u8acb', u'\\u6253', u'\\u9084', u'\\u7528', u'\\u5feb', u'\\u5927', u'\\u4f86', u'>', u'\\u5e6b', u'\\u592a', u'\\u4e00', u'\\u5565', u'<', u'\\u771f', u'\\u500b', u'+', u'\\uff5e', u'@', u'\\u548c', u'\\u6b7b', u'\\\\', u'\\u2026', u'\\u3001', u'\\u53eb', u'\\u505a', u'\\u6876', u'\\u4e2d', u'\\u641e', u'\\u539f', u'\\u7b49', u'\\u5b8c', u'\\u8036', u'%', u'\\u5f8c', u'\\u62ff', u'\\u7b11', u'\\u5148', u'\\u53ea', u'\\u8a72', u'\\u5beb', u'\\u5403', u'\\u4e8b', u'\\u7248', u'\\u5653', u'\\u5979', u'\\u6587', u'1', u'\\u5462', u'\\u5f97', u'\\u7576', u'\\u5c4c', u'\\u5272', u'\\u66f4', u'\\u4e82', u'\\u6700', u'\\u8d85', u'\\u3000', u'\\u81c9', u'\\u7d66', u'Q', u'\\u7ad9', u'\\u4e0b', u'\\u8457', u'\\u61c2', u'\\u904e', u'\\u7206', u'\\u53ef\\u4ee5', u'\\u771f\\u7684', u'\\u53f0\\u7063', u'\\u4e0d\\u662f', u'\\u5c31\\u662f', u'\\u9ad8\\u8abf', u'\\u81ea\\u5df1', u'\\u4ec0\\u9ebc', u'\\u77e5\\u9053', u'\\u51fa\\u4f86', u'\\u9019\\u6a23', u'XD', u'\\u4e0d\\u8981', u'\\u4ed6\\u5011', u'\\u52a0\\u6cb9', u'\\u6c92\\u6709', u'\\u61c9\\u8a72', u'\\u5927\\u5bb6', u'\\u73fe\\u5728', u'\\u8b66\\u5bdf', u'\\u9084\\u662f', u'\\u6211\\u5011', u'\\u4e0d\\u6703', u'\\u89ba\\u5f97', u'..', u'\\u770b\\u5230', u'\\u4e00\\u500b', u'\\u4e2d\\u570b', u'\\u652f\\u6301', u'\\u4e00\\u5b9a', u'\\u56e0\\u70ba', u'\\u600e\\u9ebc', u'\\u9019\\u7a2e', u'\\u516b\\u5366', u'\\u6709\\u4eba', u'\\u4e00\\u5806', u'\\u6c34\\u6876', u'\\u653f\\u5e9c', u'\\u5982\\u679c', u'\\u4e00\\u6a23', u'\\u6839\\u672c', u'\\u4e0d\\u904e', u'\\u9ee8\\u5de5', u'\\u53ea\\u662f', u'\\u9019\\u9ebc', u'\\u5b78\\u751f', u'\\u6bd4\\u8f03', u'\\u53ef\\u80fd', u'\\u554f\\u984c', u'\\u6240\\u4ee5', u'\\u4e0d\\u80fd', u'\\u7248\\u4e3b', u'\\u4e0d\\u7528', u'\\u5e0c\\u671b', u'\\u4e00\\u4e0b', u'\\u5f88\\u591a', u'\\u9019\\u500b', u'\\u76f4\\u63a5', u'\\u9084\\u6709', u'\\u5df2\\u7d93', u'\\u4e00\\u76f4', u'\\u4f60\\u5011', u'\\u570b\\u5bb6', u'\\u9019\\u7bc7', u'\\u6aa2\\u8209', u'\\u9109\\u6c11', u'PO', u'\\u5a92\\u9ad4', u'\\u8b1d\\u8b1d', u'\\u9019\\u4e9b', u'\\u5176\\u5be6', u'\\u90a3\\u9ebc', u'\\u4e0b\\u53f0', u'\\u670d\\u8cbf', u'\\u6c11\\u4e3b', u'\\u65b0\\u805e', u'\\u4e0d\\u7136', u'\\u63a8\\u6587', u'\\u6587\\u7ae0', u'\\u8f9b\\u82e6', u'\\u4f46\\u662f', u'\\u4eba\\u5bb6', u'\\u90a3\\u500b', u'\\u53ea\\u6709', u'\\u771f\\u662f', u'\\u8a18\\u8005', u'\\u7e7c\\u7e8c', u'\\u4e8b\\u60c5', u'\\u7d50\\u679c', u'\\u5730\\u65b9', u'\\u7684\\u8a71', u'\\u76f8\\u4fe1', u'\\u653f\\u6cbb', u'\\u611f\\u89ba', u'\\u4eba\\u6c11', u'\\u4ee5\\u5f8c', u'\\u6642\\u5019', u'\\u90a3\\u4e9b', u'\\u7f8e\\u570b', u'\\u4e4b\\u524d', u'\\u597d\\u7b11', u'\\u9019\\u662f', u'\\u5230\\u5e95', u'\\u9084\\u5728', u'\\u9053\\u6b49', u'\\u4e00\\u6b21', u'\\u9700\\u8981', u'\\u5783\\u573e', u'\\u5b8c\\u5168', u'\\u8001\\u95c6', u'\\u53ea\\u80fd', u'...', u'KMT', u'\\u570b\\u6c11\\u9ee8', u'\\u99ac\\u82f1\\u4e5d', u'\\u6709\\u6c92\\u6709', u'\\u7acb\\u6cd5\\u9662', u'XDD']\n"
     ]
    }
   ],
   "source": [
    "with open('./feature_pool/' + 'CW.txt', 'rb') as f_feature:\n",
    "    sw_list = json.load(f_feature)\n",
    "    f_feature.close()\n",
    "print sw_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data then generate user list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Trie..., from /home/joekaojoekao/github/dict.txt.big\n",
      "DEBUG:jieba:Building Trie..., from /home/joekaojoekao/github/dict.txt.big\n",
      "dumping model to file cache /tmp/jieba.user.8815186977980235228.cache\n",
      "DEBUG:jieba:dumping model to file cache /tmp/jieba.user.8815186977980235228.cache\n",
      "loading model cost 3.74733304977 seconds.\n",
      "DEBUG:jieba:loading model cost 3.74733304977 seconds.\n",
      "Trie has been built succesfully.\n",
      "DEBUG:jieba:Trie has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "999\n",
      "718\n",
      "['heavenkghs', 'nysky', 'quiet113', 'john2557', 'duo0518', 'tienhun', 'joyjack', 'vm9487', 'aftersilence', 'lingon', 'toshizo', 'yuminlin', 'fhuocrkt', 'theropod', 'kkchen', 'emptie', 'h3178378', 'zxc12385', 'Amelie27', 'buddygirl', 'chiang017', 'fromwilda', 'carters', 'Magicwind', 'glory5566', 'feather9298', 'fjdkqp', 'cojinyu', 'gayya152535', 'hnrywang', 'bugya', 'JeanSijhih', 'joyjcc', 'jeffl0402', 'Valerie06', 'SpadeR', 'cloud0528', 'xup654vu06', 'kindless', 'mylife001', 'minnie', 'ronanhuang', 'tako0988', 'treiss', 'asami', 'mineko', 'dayoa', 'akaihuang', 'sigh0602', 'wolfking623', 'koreapig5566', 'lachesis1980', 'handsomecat', 'JCS15', 'caffeine34ko', 'pprisa', 'Nick7777', 'Voldemar', 'samuelcdf', 'jaguars33', 'chx64', 'otom', 'hiro1221', 'hyde7015', 'wxtab019', 'deleteBB', 'WhyNoSmoke', 'lovemelynn', 'straight0711', 'allen5339', 'q152134', 'Sugiros', 'ursula141885', 'Monchestnut', 'captainlee', 'hoka777', 'chind', 'kaorucyc', 'surot', 'TheMiserable', 'motorolla', 'aadm', 'benson', 'colan8', 'oscar1982law', 'a0913', 'emilyluvsptt', 'Vipasyin', 'ABC9D', 'w3160828', 'cat5672', 'queue', 'gogogo12321', 'kevinptt', 'cancer0708', 'robertchun', 'neo77', 'jennywalk', 'YesGG', 'ALLYJJ2599', 'C4891', 'CHISN', 'dape321', 'morgan168', 'ccab99', 'stero', 'W96U', 'ilikebulldog', 'lordforce', 'pinkchi', 'pita30', 'seashel', 'TurTao', 'bismarcp', 'vyjssm', 'WalkFish', 'yandin', 'chachameow', 'CrazyKill', 'rufjvm12345', 'nicest', 'orz151426', 'lo23', 'ittie', 'Automatic620', 'Leaves827', 'Meow0129', 'hydeaya', 'forRITZ', 'alentek', 'roxcido', 'hiphopphysic', 'RLH', 'iloveshida', 'xyz2', 'yovven', 'rookiebear', 'arhuro', 'genka', 'a129517496', 'codehard', 'qaz963747', 'TaTa5566', 'ahao1105', 'caesar119', 'coolwind4410', 'foxfox1017', 'pppeeeppp', 'purplebfly', 'pennymarkfox', 'VoyagerKid', 'Allen0315', 'comjj45', 'sukisam', 'qkenny', 'y5gogogo', 'fcu6969', 'hicker', 'libraayu', 'badbadook', 'awoorog', 'shinjyeh', 'tooeasy', 'rabbit83035', 'wt5665', 'bewiiarro', 'pljck', 'BiBiG', 'haijin', 'omegajoker', 'coolfuc', 'asterisk0213', 'deepmilk', 'hot5566', 'HNO3', 'yangnana', 'ai4zo', 'Karajan8305', 'HAHAHUNG', 'JJLi', 'Nerv', 'FIRZEN45', 'wanying', 'liiiiiiiio', 'wolver', 'LIONDODO', 'lupolewis', 'airstation', 'matthew0123', 'luxifer', 'signm', 'crazycomet', 'AAAC', 'AAAB', 'AAAD', 'li751012', 'aoiaoi', 'southring', 'andy80209', 'u831208', 'alyce36', 'for103you', 'coolman123', 'jdemon', 'jil', 'Azu', 'destroyed', 'BlueJet0501', 'uok', 'herman602', 'bc1007', 'zaqmkovfr', 'miniamigo', 'wilsonno1', 'KTA0619', 'DarkKnight', 'liangmei', 'douglasyeh', 'liao18', 'kiki2125', 'sleepybaby', 'tivoli0315', 'Yakyuboy51', 'shhh', 'ph4586', 'Akaiito', 'NTRyourwife', 'hickory', 'TAKIHERO', 'pilot1982', 'sasamotorena', 'ninashu73121', 'nacciabe', 'a23633302', 'hirokofan', 'qzp5408', 's952112', 'nanie', 'traystien', 'staroceanj9', 'liyimu', 'gotohikaru', 'Mchord', 'erixerix', 'hillkoko', 'showken', 'hem0207', 'yoyoman0529', 'justin9012', 'lovesuicide', 'adapt', 'ronlai', 'chifu', 'e04su3fn3', 'FTKBOYS', 'melidy', 'ri', 'Tmgy', 'Safin', 'chivalry70', 'wht810090', 'ckshchen', 'newstarting', 'mykuririn', 'jane26thmix', 'zxcvf', 'GodsRevival', 'james780909', 'SundayRose', 'casperrrr', 'rca0621', 'goldman0204', 'hamel', 'benboman', 'liyuoh', 'wowisgood', 'weifish', 'hapteasing', 'bpooqd', 'biglafu', 'finzaghi', 'hankwu', 'hb000', 'tn00706223', 'genius0716', 'niceright', 'hohaiyanyan', 'nobeldd', 'meaning12', 'a1341150854', 'catbuji', 'Fezico', 'jelly1128', 'john07', 'looop', 'supermars', 'lauly', 'akimiyavi', 'HOWARDSHEN', 'snian', 'qwaqwa2007', 'rotusea', 'heisego', 'YOYOGIVEME', 'hydralee', 'wangisiung', 'hownever', 'daleleu', 'jyue', 'ainosei', 'iamursis', 'happyqaz', 'zhvul3u4', 'ryokoon', 'stilllove56', 'heptachord', 'edu94848325', 'YumingHuang', 'rainingdayz', 'dannyxm3', 'opjinshan', 'icead', 'wl1451226', 'nzj', 'DEUT', 'cherish772', 'fresheasy', 'BeefNoodles', 'jokywolf', 'doom3', 'freeway11', 'TheJ', 'WatanabeKen', 'newjackle', 'wallowes', 'yuhurefu', 'grahamwu', 'catdreaming', 'everoh99', 'catvsdog', 'kaojet', 'RaHarakhte', 'JPMontoya', 'charlie0531', 'guesttseug', 'eljin', 'zop', 'nfish', 'goodman', 'applejone', 'Invec', 'kontrollCat', 'GreenScatter', 'ZMittermeyer', 'keithking', 'taiwanwang', 'jeans520', 'breadflower', 'wujason', 'BBBBBBBB', 'sluttysavage', 'omaha', 'hollowyears', 'jessicakard', 'SNGoMMX', 'shippai', 'lockgolden', 'mia3', 'ATONG25', 'phix', 'lunlun0802', 'yule1224', 'blackteasart', 'a9wh61ks', 'nelljin', 'Infernity', 'jopyt', 'ioupoiu', 'doras', 'MiniiH', 'JoyceSmile', 'erptt', 'asukace', 'chosenone', 'blacktea5566', 'lepidoptera', 'Barquinho', 'makapaka', 'cappin', 'cyswu', 'gg123sf', 'QUEENSUSAN', 'hsgreent', 'mailismine', 'yozeng', 'm0630821', 'longmok2500', 'Ga11ardo', 'pppttt', 'Terrykho', 'ariesw', 'jknm0510a', 'raxy', 'a22363491', 'MaRiaNi', 'rythem', 'tsaiyilun307', 'olgas', 'ddlnm', 'toro736', 'Biganan', 'punksion', 'awer89', 'FatDaniel', 'bigheadpro', 'HenryTudor', 'Bookdaily', 'will8149', 'jabari', 'Gnome', 'teaplus', 'josLynYa', 'tume7', 'cpuds', 'xul327', 'MAGGIE99', 'sunface', 'asbbey', 'wildgoat', 'AMPHIBIA', 'cappa', 'liam5184910', 'Darvish816', 'rogergon', 'tfoasy', 'zainlove', 'girl10319', 'best2008', 'ams9', 'ImMACACO', 'tsaiyunlung', 'VGA', 'jkasc28s', 'probsk', 'q224222', 'airflow', 'moocow', 'cwjing', 'gmkuo', 'baby1022', 'mobile02', 'jasonrod05', 'howdiee', 'nfstako', 'olli', 'Kay731', 'Dusha', 'adoken', 'ollo', 'underload', 'dodonpachi', 'blackcellar', 'sheng3476', 'kirby0415', 'mayurina', 'Smile365Day', 'b0d', 'EV1L', 'a2629397', 'lomgray', 'iosian', 'pradalove', 'nipon0208', 'Vett', 'jack33', 'canser', 'blackcateva', 'kobejo4', 'GoodElephant', 'sggs', 'typekid', 'Gloom666', 'regun', 'creulfact', 'majanliu', 'jimli', 'wk105', 'ID556', 'shena30335', 'MortalCGU', 'smpian', 'CO2', 'aerymani', 'tn00210585', 'stemcell', 'zzj11', 'S80', 'flyinwind828', 'eeee111', 'swordfish217', 'APPLEin5566', 'osak', 'LakeBodom', 'andersontom', 'yukina23', 'henrry60109', 'xian', 'sean60106', 'eterbless', 'ryan0222', 'ChosYon', 'hanhsiangmax', 'ascii', 'saeea', 's4340392', 'koriras', 'aurorax', 'mit28', 'lilieye', 'crossover103', 'yTim5566', 'maxdi', 'ciplu', 'yjchiou', 'manmanhuang', 'shiva999', 'miyuika', 'zenixls2', 'purpose', 'XVN', 'oscar721', 'kula77', 'phil1984', 'cain07', 'stfang925', 'rangting', 'batt0512', 'Hans14', 'DarkerWu', 'STEVENUA', 'rock666', 'ymca900', 'asukaka', 'jenny830913', 'benza', 'hihisnoopy', 'enoeno', 'arsure666', 'didolydia', 'jojoSpirit', 'ohmickey', 'iceworld25', 'appleswill', 'vjuko', 'ncnuboy', 'groundmon', 'gn780802', 'CORSA', 'Marino', 'ichbinsarah', 'flowersuger', 'ufoon', 'louis5265', 'itsmyspirit', 'Invincibleme', 'teddygoodgoo', 'anoreader', 'yragggc', 'Miseryz', 'oneyear', 'istay', 'q13461346', 'kissbin', 'Tenging', 'Sousake', 'a710689', 'bced', 'cashayoung', 'larrysu', 'cofc', 'EN23', 'CTgogogo', 'homerunball', 'oeibei', 'Hall', 'marques', 'vdan', 'fukobe', 'jplo', 'rettttt5', 'diego99', 'coolwei', 'hanklun', 'shenasu', 'yuyuyou', 'pttdoris', 'LUB500', 'jmrla', 'RHTZ', 'kakaman', 'redstone', 'terry456', 'eric61446', 'kiddno56979', 'pz5202', 's1020824', 'handfoxx', 'johnny3', 'scottlsw', 'qqu7799tw', 'newwu', 'unclefucka', 'eddy1221', 'dakook', 'heloiselu', 'goetze', 'MattiaPasini', 'Akhenaten', 'u5710587', 'meowzilla', 'V6WOLF', 'RPedsel', 'et803', 'ysk999', 'takepron', 'toy812', 'johnnyd', 'boy00225', 'Kakehiko', 'poemking', 'southboy', 'ACrimsonTide', 'pomelo524', 'ariel0912', 'lucef', 'acer758219', 'poipoi882002', 'machida', 'crazymome', 'rock123520', 'zeta731115', 'antje1211', 'rusa', 'bjj', 'liebes', 'ZEALOTGO', 'addycat', 'itrs821', 'atom1130', 'maggiekiki', 'xx60824xx', 'izacc', 'sunkis6842', 'iMANIA', 'cuckooflower', 'opthr1215', 'Leepofeng', 'Carmelo3', 'yaya', 'Amadeus1008', 'diefox', 'Feuerbach', 'peiring', 'luciffar', 'appovoa', 's1385999', 'a1038874', 'longlyplant', 'yun3liu', 'eitingirl', 'MasCat', 'HIRU', 'BlackBass', 'julia66', 'AQUANGEL', 'puec2', 'bimmer3', 'snow71412', 'minan', 'popy8789', 'zankarasu', 'KOBER81', 'ohmygod0707', 'benjaminchia', 'creak', 'testmac', 'WeAntiTVBS', 'MagicMoney', 'vovzz', 'setraise', 'chucky', 'yogi', 'rs6000', 'REALLYLIFE', 'fansboy', 'wild2012', 'gaiaesque', 'sunnywei', 'fst1985', 'sorryfly', 'dodokevin', 'slx54461', 'iamOsaka', 'ytall', 'daidai', 'sizhen', 'lulumic', 'eva617266', 'hsinwen', 'mmpercussion', 'whiterM', 'kingdef', 'Cheese27', 'lch2011', 'euphoria01', 'sobadwhos', 'jack0204', 'excel5566']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "##import processed push data\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_dir = config[\"sample_dir\"]\n",
    "sample_file = config[\"sample_file\"] \n",
    "TEST_SIZE = config[\"TEST_SIZE\"] \n",
    "temp_path = config[\"temp_path\"]\n",
    "\n",
    "#/Users/joekaojoekao/PycharmProjects/push/github/visualized/select_pushes1000_1.txt\n",
    "with open(sample_dir + sample_file, 'rb') as fin:\n",
    "    s = fin.read().split('\\n')\n",
    "    lines = []\n",
    "    for line in s:\n",
    "        lines.append(line.split('\\t'))\n",
    "    fin.close()\n",
    "\n",
    "sample_dict = {}\n",
    "for i in lines:\n",
    "    \n",
    "    #print len(i)\n",
    "    #if len(i) >= 3:\n",
    "    if len(i) != 2:\n",
    "        continue\n",
    "    uid = i[0]\n",
    "    push = i[1]\n",
    "    if uid not in sample_dict:\n",
    "        sample_dict[uid] = [push]\n",
    "    else:\n",
    "        sample_dict[uid].append(push)\n",
    "#print sample_dict\n",
    "\n",
    "\n",
    "##segment pushes\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "jieba.analyse.set_stop_words('stopword_pool/merged_stopword.txt')\n",
    "\n",
    "sample_dict_jieba = {}\n",
    "for uid in sample_dict.keys():\n",
    "    pushes = sample_dict[uid]\n",
    "    if len(pushes) > 0:\n",
    "        sample_dict_jieba[uid] = []\n",
    "        for push in pushes:\n",
    "            seg_list = list(jieba.cut(push, cut_all=False))\n",
    "            sample_dict_jieba[uid].append((push,seg_list))\n",
    "#從ptt push開始處理\n",
    "#我要uid:push&trunked word\n",
    "#幹 要現切 因為產生的東西只剩uid+push(無順序無法查)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "#set gt_numword (meaningful), gt_numpush (active)\n",
    "NUM_WORD_GT = 4\n",
    "NUM_PUSH_GT = 3 #for 6 fold-cv\n",
    "\n",
    "new_pushes = {}\n",
    "for k in sample_dict.keys():\n",
    "    gt_push = [x for x in sample_dict[k] if len(x) > NUM_WORD_GT]\n",
    "    if len(gt_push) > 0:\n",
    "        new_pushes[k] = gt_push\n",
    "\n",
    "user_list = []\n",
    "for k in new_pushes.keys():\n",
    "    if len(new_pushes[k]) >= NUM_PUSH_GT:\n",
    "        user_list.append(k)\n",
    "        \n",
    "print len(sample_dict)\n",
    "print len(new_pushes)\n",
    "print len(user_list)\n",
    "print user_list\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "#temp_path = '/Users/joekaojoekao/PycharmProjects/push/github/temp/'\n",
    "#sample_file = 'select_pushes101'\n",
    "#write in temp\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt', 'wb') as f_temp:\n",
    "    f_temp.write(json.dumps(user_list, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "    f_temp.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##generate cv lists for user and ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gen_cv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gen_cv.py\n",
    "#from sklearn import cross_validation\n",
    "#CV_TIMES = 3\n",
    "\n",
    "#list_user = [{} for i in range(3)] \n",
    "#list_user_ans = [{} for i in range(3)] \n",
    "user = {}\n",
    "user_ans = {}\n",
    "\n",
    "\n",
    "def crossover_split(training_set):\n",
    "    odd = training_set[1::2]\n",
    "    even = training_set[::2]\n",
    "    return odd, even\n",
    "\n",
    "for uid in user_list:\n",
    "    training_set = sample_dict_jieba[uid]\n",
    "    #cv = cross_validation.ShuffleSplit(len(training_set), n_iter=CV_TIMES, test_size=TEST_SIZE) # 2/3 for train, 1/3 for test, do 3 times  \n",
    "    #do crossover split\n",
    "    train, test = crossover_split(training_set)\n",
    "    \n",
    "    #for n, (traincv, testcv) in enumerate(cv): #每個user都有3組cv的推文\n",
    "        \n",
    "        #train_list = [training_set[i] for i in traincv]\n",
    "        #test_list = [training_set[i] for i in testcv]\n",
    "        \n",
    "        #list_user[n][uid] = train_list\n",
    "        #list_user_ans[n][uid] = test_list\n",
    "    user[uid] = train\n",
    "    user_ans[uid] = test\n",
    "\n",
    "import json\n",
    "\n",
    "#temp_path = '/Users/joekaojoekao/PycharmProjects/push/github/temp/'\n",
    "#sample_file = 'select_pushes101'\n",
    "#temp_json = (list_user, list_user_ans)\n",
    "temp_json = (user, user_ans)\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist_new.txt', 'wb') as f_temp:\n",
    "    f_temp.write(json.dumps(temp_json, indent=2, ensure_ascii=True).encode('utf-8'))\n",
    "    f_temp.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##do cv with pypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word 9507\n",
      "stop word 249\n",
      "rare word 6073\n",
      "num_word: 1 \t132\n",
      "num_word: 2 \t107\n",
      "num_word: 3 \t7\n",
      "num_word: 4 \t2\n",
      "num_word: 5 \t1\n",
      "249\n",
      "249\n",
      "249\n",
      "718\n",
      "...\n",
      "[ 0.25792812  0.04989605  0.04744526 ...,  0.125       0.07266122\n",
      "  0.08080808]\n",
      "Time =  0.856564998627 s\n"
     ]
    }
   ],
   "source": [
    "%%pypy\n",
    "import json\n",
    "import numpypy as np\n",
    "import time\n",
    "\n",
    "def count_dict(sample_dict_jieba):\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "    import itertools\n",
    "    ptt_pushes_freq_bypush = {}\n",
    "    for uid, push_list in sample_dict_jieba.iteritems():\n",
    "        push = push_list[0]\n",
    "        a = zip(*push_list)[1]\n",
    "        push_gram = list(itertools.chain(*a))\n",
    "        count = Counter(push_gram)\n",
    "        temp = []\n",
    "        for w, c in count.most_common():\n",
    "            temp.append((w, c))\n",
    "        id_count = Counter(dict(temp))\n",
    "        ptt_pushes_freq_bypush[uid] = dict(id_count)\n",
    "\n",
    "    return ptt_pushes_freq_bypush\n",
    "\n",
    "def weighted_jaccard(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        return -1\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in xrange(len(l1)):\n",
    "        num += np.minimum(l1[i], l2[i])\n",
    "        den += np.maximum(l1[i], l2[i])\n",
    "    wj = np.divide(np.float64(num), den)\n",
    "\n",
    "    return wj\n",
    "\n",
    "\n",
    "#following need to set in config.txt\n",
    "#read from config file again since suck pypy magic\n",
    "\n",
    "import os\n",
    "dir_path = os.getcwd() + '/'\n",
    "with open(dir_path + 'config.txt', 'rb') as f_conf:\n",
    "    config = json.load(f_conf)\n",
    "    f_conf.close()\n",
    "    \n",
    "sample_file = config[\"sample_file\"] \n",
    "temp_path = config[\"temp_path\"]\n",
    "result_dir = config[\"result_dir\"]\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_userlist.txt', 'rb') as f_temp:\n",
    "    user_list = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "\n",
    "with open(temp_path + os.path.splitext(sample_file)[0] + '_cvlist_new.txt', 'rb') as f_temp:\n",
    "    temp_json = json.load(f_temp)\n",
    "    f_temp.close()\n",
    "\n",
    "dict_user = temp_json[0] # train push list\n",
    "dict_user_ans = temp_json[1] # test push list\n",
    "dict_user_count = count_dict(dict_user) #count the freq    \n",
    "dict_user_count_ans = count_dict(dict_user_ans) #count the freq for ans(test)\n",
    "from collections import Counter\n",
    "count_all = Counter()\n",
    "for v in dict_user_count.values():\n",
    "    count_all += Counter(v)\n",
    "\n",
    "from collections import OrderedDict\n",
    "dict_all_count = OrderedDict(sorted(dict(count_all).items(), key=lambda t: t[1], reverse=True))\n",
    "\n",
    "##gen sw (but only from training data)\n",
    "word = dict_all_count.keys()\n",
    "W_PERCENT = 0.025 #0.025\n",
    "## setting for stopword & rareword percentage\n",
    "stopwords = [x for x in word if dict_all_count[x] >= dict_all_count[word[int(len(word) * W_PERCENT)]]]\n",
    "rarewords = [x for x in word if dict_all_count[x] <= dict_all_count[word[int(len(word) * (1-W_PERCENT))]]] #0.975\n",
    "\n",
    "## stop word list\n",
    "sw_list = [x for x in stopwords] \n",
    "## rare word list\n",
    "rw_list = [x for x in rarewords] \n",
    "print 'total word', len(word)\n",
    "print 'stop word', len(sw_list)\n",
    "print 'rare word', len(rw_list)\n",
    "\n",
    "\n",
    "sw_count_dict = {}\n",
    "for w in sw_list:\n",
    "    sw_count_dict.setdefault(len(w),[]).append(w)\n",
    "\n",
    "\n",
    "K_NUMWORD = 6\n",
    "selected_sw = []\n",
    "for k in sw_count_dict.keys()[0:K_NUMWORD]:\n",
    "    print 'num_word:',k,'\\t',len(sw_count_dict[k])\n",
    "    selected_sw += sw_count_dict[k]\n",
    "print len(selected_sw)\n",
    "\n",
    "\n",
    "\n",
    "##gen vec\n",
    "import itertools\n",
    "general_vec = {}\n",
    "for uid in user_list: # for each user id\n",
    "    user_len = sum([len(x) for x in dict_user_count[uid]]) #total word freq\n",
    "    if user_len > 0:\n",
    "        vec = [dict_user_count[uid].get(w, 0) for w in selected_sw]\n",
    "        g_vec = [float(x) / user_len for x in vec]\n",
    "        general_vec[uid] = g_vec\n",
    "\n",
    "\n",
    "general_vec_ans = {}\n",
    "for uid in user_list: # for each user id\n",
    "    user_len = sum([len(x) for x in dict_user_count_ans[uid]]) #total word freq\n",
    "    #print sum(v.values())\n",
    "    if user_len > 0:\n",
    "        vec = [dict_user_count_ans[uid].get(w, 0) for w in selected_sw]\n",
    "        g_vec = [float(x) / user_len for x in vec]\n",
    "        general_vec_ans[uid] = g_vec\n",
    "\n",
    "print len(general_vec[user_list[0]])\n",
    "print len(general_vec_ans[user_list[0]])\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "user_size = len(user_list)\n",
    "print user_size\n",
    "\n",
    "sim_list = np.array(np.arange(user_size*user_size))\n",
    "sim_list = sim_list.astype(float)\n",
    "print '...'\n",
    "idx = 0\n",
    "for i in xrange(user_size):\n",
    "    for j in xrange(user_size):\n",
    "        #wj_sw = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "        #sim_list = np.vstack((sim_list, np.array((round(float(i),1), round(float(j),1), wj_sw))))\n",
    "\n",
    "        sim_list[idx] = weighted_jaccard(general_vec[user_list[i]], general_vec_ans[user_list[j]])\n",
    "        idx += 1\n",
    "sim_list = sim_list[:idx]\n",
    "t_stop = time.time()\n",
    "print sim_list\n",
    "print 'Time = ', t_stop - t_start, 's'\n",
    "\n",
    "user_sim_list = []\n",
    "idx = 0\n",
    "for i in xrange(user_size):\n",
    "    for j in xrange(user_size):\n",
    "        sim = sim_list[idx]\n",
    "        idx += 1\n",
    "        user_sim_list.append((user_list[int(i)], user_list[int(j)] + 'ANS', sim))\n",
    "#name = os.path.splitext(sample_file)[0]\n",
    "with open(result_dir + os.path.splitext(sample_file)[0] + '_cv.txt', 'wb') as fout:\n",
    "    for user, ans, sim in user_sim_list:\n",
    "        line = user + ',' + ans + ',' + str(sim) +'\\n'\n",
    "        fout.write(line.encode('utf-8'))\n",
    "    fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
